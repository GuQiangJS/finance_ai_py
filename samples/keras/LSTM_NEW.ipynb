{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "import QUANTAXIS as QA\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pyecharts\n",
    "import talib\n",
    "\n",
    "#设定绘图的默认大小\n",
    "import matplotlib\n",
    "matplotlib.rcParams[\"figure.figsize\"]=[16,5]\n",
    "\n",
    "matplotlib.rcParams['font.family'] = 'sans-serif'\n",
    "matplotlib.rcParams['font.sans-serif'] = ['Noto Sans CJK SC','SimHei']\n",
    "matplotlib.rcParams['axes.unicode_minus']=False #用来正常显示负号\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#加载 seaborn，并且设置默认使用 seaborn\n",
    "import seaborn as sns\n",
    "sns.set(font=['Noto Sans CJK SC','SimHei'])\n",
    "\n",
    "# from IPython.core.interactiveshell import InteractiveShell\n",
    "# InteractiveShell.ast_node_interactivity = \"all\" \n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten, LSTM, TimeDistributed, RepeatVector\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>code</th>\n",
       "      <th>open</th>\n",
       "      <th>high</th>\n",
       "      <th>low</th>\n",
       "      <th>close</th>\n",
       "      <th>volume</th>\n",
       "      <th>amount</th>\n",
       "      <th>preclose</th>\n",
       "      <th>adj</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2006-10-27</th>\n",
       "      <td>601398</td>\n",
       "      <td>1.992133</td>\n",
       "      <td>2.01557</td>\n",
       "      <td>1.910104</td>\n",
       "      <td>1.921823</td>\n",
       "      <td>4.407654e+07</td>\n",
       "      <td>8.725310e+09</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.585922</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              code      open     high       low     close        volume  \\\n",
       "date                                                                      \n",
       "2006-10-27  601398  1.992133  2.01557  1.910104  1.921823  4.407654e+07   \n",
       "\n",
       "                  amount  preclose       adj  \n",
       "date                                          \n",
       "2006-10-27  8.725310e+09       NaN  0.585922  "
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stock_code='601398'\n",
    "benchmark_code='399300'\n",
    "start_time='2005-01-01'\n",
    "end_time='2018-12-31'\n",
    "\n",
    "data_raw=QA.QA_fetch_stock_day_adv(stock_code, start_time, end_time).to_qfq().data.reset_index().set_index('date')\n",
    "data_raw.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>open</th>\n",
       "      <th>high</th>\n",
       "      <th>low</th>\n",
       "      <th>close</th>\n",
       "      <th>volume</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2006-10-27</th>\n",
       "      <td>1.992133</td>\n",
       "      <td>2.01557</td>\n",
       "      <td>1.910104</td>\n",
       "      <td>1.921823</td>\n",
       "      <td>4.407654e+07</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                open     high       low     close        volume\n",
       "date                                                           \n",
       "2006-10-27  1.992133  2.01557  1.910104  1.921823  4.407654e+07"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data=data_raw.drop(columns=['code','amount','preclose','adj'])\n",
    "data.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 对数据进行包装\n",
    "\n",
    "### 增加特性数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def augFeatures(train):\n",
    "    df=train.copy()\n",
    "    df[\"year\"] = df.index.year\n",
    "    df[\"month\"] = df.index.month\n",
    "    df[\"date\"] = df.index.day\n",
    "    df[\"day\"] = df.index.dayofweek\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 正则化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(train):\n",
    "    return train.apply(lambda x: (x - np.mean(x)) / (np.max(x) - np.min(x)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 其他方法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "def buildData(df,pastDays=30,futureDays=5):\n",
    "    \"\"\"取 `pastDays` 的数据作为计算数据，取 `futureDays` 的 `close` %save据作为测算数据。\n",
    "    返回的X集合中包含类型为 `DataFrame` ，Y集合中包含为 `Series`。\"\"\"\n",
    "    X,Y=[],[]\n",
    "    for i in range(df.shape[0]-pastDays-futureDays):\n",
    "        X.append(df.iloc[i:i+pastDays])\n",
    "        Y.append(df.iloc[i+pastDays:i+pastDays+futureDays]['close'])\n",
    "    return X,Y\n",
    "\n",
    "# def splitBuildData(X,Y,test_size=0.2,random_state=10,shuffle=True):\n",
    "#     \"\"\"调用 `sklearn.model_selection.train_test_split` callable分训练集和测试集。\n",
    "#     Args:\n",
    "#         shuffle: 是否打乱数据。默认为 `True`。\n",
    "        \n",
    "#     Returns:\n",
    "#         返回内容为：X_train, X_test, y_train, y_test。\n",
    "#         其中 X_train和y_train配对，X_test和y_test配对。\"\"\"\n",
    "#     return train_test_split(X,Y,test_size=test_size,random_state=random_state,shuffle=shuffle)\n",
    "\n",
    "def toNpArray(d):\n",
    "    return np.array([np.array(x.values) for x in d])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 数据演示"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data_raw.drop(columns=['code','amount','preclose','adj'])\n",
    "# 包装数据\n",
    "data_Aug = augFeatures(data)\n",
    "# 归一化数据\n",
    "data_norm = normalize(data_Aug)\n",
    "# 构建结果集\n",
    "X,Y=buildData(data_norm)\n",
    "# 按顺序拆分数据和结果\n",
    "X_train, X_test, y_train, y_test = splitBuildData(X, Y,shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>open</th>\n",
       "      <th>high</th>\n",
       "      <th>low</th>\n",
       "      <th>close</th>\n",
       "      <th>volume</th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "      <th>date</th>\n",
       "      <th>day</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2006-10-27</th>\n",
       "      <td>-0.290901</td>\n",
       "      <td>-0.286750</td>\n",
       "      <td>-0.305056</td>\n",
       "      <td>-0.297344</td>\n",
       "      <td>0.947017</td>\n",
       "      <td>-0.535039</td>\n",
       "      <td>0.303473</td>\n",
       "      <td>0.362010</td>\n",
       "      <td>0.497366</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2006-10-30</th>\n",
       "      <td>-0.305110</td>\n",
       "      <td>-0.299520</td>\n",
       "      <td>-0.306172</td>\n",
       "      <td>-0.296279</td>\n",
       "      <td>0.080294</td>\n",
       "      <td>-0.535039</td>\n",
       "      <td>0.303473</td>\n",
       "      <td>0.462010</td>\n",
       "      <td>-0.502634</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2006-10-31</th>\n",
       "      <td>-0.304017</td>\n",
       "      <td>-0.298456</td>\n",
       "      <td>-0.302826</td>\n",
       "      <td>-0.295213</td>\n",
       "      <td>0.032970</td>\n",
       "      <td>-0.535039</td>\n",
       "      <td>0.303473</td>\n",
       "      <td>0.495343</td>\n",
       "      <td>-0.252634</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2006-11-01</th>\n",
       "      <td>-0.301831</td>\n",
       "      <td>-0.300584</td>\n",
       "      <td>-0.302826</td>\n",
       "      <td>-0.295213</td>\n",
       "      <td>-0.004811</td>\n",
       "      <td>-0.535039</td>\n",
       "      <td>0.394382</td>\n",
       "      <td>-0.504657</td>\n",
       "      <td>-0.002634</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2006-11-02</th>\n",
       "      <td>-0.301831</td>\n",
       "      <td>-0.301648</td>\n",
       "      <td>-0.306172</td>\n",
       "      <td>-0.297344</td>\n",
       "      <td>0.011611</td>\n",
       "      <td>-0.535039</td>\n",
       "      <td>0.394382</td>\n",
       "      <td>-0.471323</td>\n",
       "      <td>0.247366</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2006-11-03</th>\n",
       "      <td>-0.305110</td>\n",
       "      <td>-0.294199</td>\n",
       "      <td>-0.303941</td>\n",
       "      <td>-0.288819</td>\n",
       "      <td>0.098982</td>\n",
       "      <td>-0.535039</td>\n",
       "      <td>0.394382</td>\n",
       "      <td>-0.437990</td>\n",
       "      <td>0.497366</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2006-11-06</th>\n",
       "      <td>-0.298552</td>\n",
       "      <td>-0.293135</td>\n",
       "      <td>-0.298366</td>\n",
       "      <td>-0.286688</td>\n",
       "      <td>0.025147</td>\n",
       "      <td>-0.535039</td>\n",
       "      <td>0.394382</td>\n",
       "      <td>-0.337990</td>\n",
       "      <td>-0.502634</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2006-11-07</th>\n",
       "      <td>-0.293087</td>\n",
       "      <td>-0.287814</td>\n",
       "      <td>-0.292790</td>\n",
       "      <td>-0.284557</td>\n",
       "      <td>0.058874</td>\n",
       "      <td>-0.535039</td>\n",
       "      <td>0.394382</td>\n",
       "      <td>-0.304657</td>\n",
       "      <td>-0.252634</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2006-11-08</th>\n",
       "      <td>-0.290901</td>\n",
       "      <td>-0.291007</td>\n",
       "      <td>-0.295020</td>\n",
       "      <td>-0.288819</td>\n",
       "      <td>0.002545</td>\n",
       "      <td>-0.535039</td>\n",
       "      <td>0.394382</td>\n",
       "      <td>-0.271323</td>\n",
       "      <td>-0.002634</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2006-11-09</th>\n",
       "      <td>-0.295273</td>\n",
       "      <td>-0.291007</td>\n",
       "      <td>-0.295020</td>\n",
       "      <td>-0.284557</td>\n",
       "      <td>0.012696</td>\n",
       "      <td>-0.535039</td>\n",
       "      <td>0.394382</td>\n",
       "      <td>-0.237990</td>\n",
       "      <td>0.247366</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2006-11-10</th>\n",
       "      <td>-0.290901</td>\n",
       "      <td>-0.278237</td>\n",
       "      <td>-0.289445</td>\n",
       "      <td>-0.278163</td>\n",
       "      <td>0.238736</td>\n",
       "      <td>-0.535039</td>\n",
       "      <td>0.394382</td>\n",
       "      <td>-0.204657</td>\n",
       "      <td>0.497366</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2006-11-13</th>\n",
       "      <td>-0.285436</td>\n",
       "      <td>-0.270787</td>\n",
       "      <td>-0.284984</td>\n",
       "      <td>-0.271770</td>\n",
       "      <td>0.123600</td>\n",
       "      <td>-0.535039</td>\n",
       "      <td>0.394382</td>\n",
       "      <td>-0.104657</td>\n",
       "      <td>-0.502634</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2006-11-14</th>\n",
       "      <td>-0.271227</td>\n",
       "      <td>-0.267595</td>\n",
       "      <td>-0.274948</td>\n",
       "      <td>-0.263245</td>\n",
       "      <td>0.111375</td>\n",
       "      <td>-0.535039</td>\n",
       "      <td>0.394382</td>\n",
       "      <td>-0.071323</td>\n",
       "      <td>-0.252634</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2006-11-15</th>\n",
       "      <td>-0.267948</td>\n",
       "      <td>-0.264402</td>\n",
       "      <td>-0.267142</td>\n",
       "      <td>-0.257917</td>\n",
       "      <td>0.077452</td>\n",
       "      <td>-0.535039</td>\n",
       "      <td>0.394382</td>\n",
       "      <td>-0.037990</td>\n",
       "      <td>-0.002634</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2006-11-16</th>\n",
       "      <td>-0.258111</td>\n",
       "      <td>-0.234606</td>\n",
       "      <td>-0.259337</td>\n",
       "      <td>-0.241932</td>\n",
       "      <td>0.210768</td>\n",
       "      <td>-0.535039</td>\n",
       "      <td>0.394382</td>\n",
       "      <td>-0.004657</td>\n",
       "      <td>0.247366</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2006-11-17</th>\n",
       "      <td>-0.248274</td>\n",
       "      <td>-0.243119</td>\n",
       "      <td>-0.251531</td>\n",
       "      <td>-0.241932</td>\n",
       "      <td>0.065887</td>\n",
       "      <td>-0.535039</td>\n",
       "      <td>0.394382</td>\n",
       "      <td>0.028677</td>\n",
       "      <td>0.497366</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2006-11-20</th>\n",
       "      <td>-0.247181</td>\n",
       "      <td>-0.235670</td>\n",
       "      <td>-0.244840</td>\n",
       "      <td>-0.229145</td>\n",
       "      <td>0.067133</td>\n",
       "      <td>-0.535039</td>\n",
       "      <td>0.394382</td>\n",
       "      <td>0.128677</td>\n",
       "      <td>-0.502634</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2006-11-21</th>\n",
       "      <td>-0.235158</td>\n",
       "      <td>-0.231413</td>\n",
       "      <td>-0.238149</td>\n",
       "      <td>-0.228080</td>\n",
       "      <td>0.034110</td>\n",
       "      <td>-0.535039</td>\n",
       "      <td>0.394382</td>\n",
       "      <td>0.162010</td>\n",
       "      <td>-0.252634</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2006-11-22</th>\n",
       "      <td>-0.235158</td>\n",
       "      <td>-0.232477</td>\n",
       "      <td>-0.243725</td>\n",
       "      <td>-0.231276</td>\n",
       "      <td>0.053692</td>\n",
       "      <td>-0.535039</td>\n",
       "      <td>0.394382</td>\n",
       "      <td>0.195343</td>\n",
       "      <td>-0.002634</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2006-11-23</th>\n",
       "      <td>-0.234065</td>\n",
       "      <td>-0.226092</td>\n",
       "      <td>-0.231459</td>\n",
       "      <td>-0.223817</td>\n",
       "      <td>0.061264</td>\n",
       "      <td>-0.535039</td>\n",
       "      <td>0.394382</td>\n",
       "      <td>0.228677</td>\n",
       "      <td>0.247366</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2006-11-24</th>\n",
       "      <td>-0.234065</td>\n",
       "      <td>-0.233541</td>\n",
       "      <td>-0.238149</td>\n",
       "      <td>-0.229145</td>\n",
       "      <td>0.017078</td>\n",
       "      <td>-0.535039</td>\n",
       "      <td>0.394382</td>\n",
       "      <td>0.262010</td>\n",
       "      <td>0.497366</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2006-11-27</th>\n",
       "      <td>-0.237344</td>\n",
       "      <td>-0.237798</td>\n",
       "      <td>-0.249301</td>\n",
       "      <td>-0.240867</td>\n",
       "      <td>0.060302</td>\n",
       "      <td>-0.535039</td>\n",
       "      <td>0.394382</td>\n",
       "      <td>0.362010</td>\n",
       "      <td>-0.502634</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2006-11-28</th>\n",
       "      <td>-0.249367</td>\n",
       "      <td>-0.247376</td>\n",
       "      <td>-0.254876</td>\n",
       "      <td>-0.247261</td>\n",
       "      <td>0.037753</td>\n",
       "      <td>-0.535039</td>\n",
       "      <td>0.394382</td>\n",
       "      <td>0.395343</td>\n",
       "      <td>-0.252634</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2006-11-29</th>\n",
       "      <td>-0.261390</td>\n",
       "      <td>-0.248440</td>\n",
       "      <td>-0.261567</td>\n",
       "      <td>-0.242998</td>\n",
       "      <td>0.038482</td>\n",
       "      <td>-0.535039</td>\n",
       "      <td>0.394382</td>\n",
       "      <td>0.428677</td>\n",
       "      <td>-0.002634</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2006-11-30</th>\n",
       "      <td>-0.247181</td>\n",
       "      <td>-0.245247</td>\n",
       "      <td>-0.248185</td>\n",
       "      <td>-0.240867</td>\n",
       "      <td>0.033632</td>\n",
       "      <td>-0.535039</td>\n",
       "      <td>0.394382</td>\n",
       "      <td>0.462010</td>\n",
       "      <td>0.247366</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2006-12-01</th>\n",
       "      <td>-0.243902</td>\n",
       "      <td>-0.245247</td>\n",
       "      <td>-0.251531</td>\n",
       "      <td>-0.246195</td>\n",
       "      <td>0.050096</td>\n",
       "      <td>-0.535039</td>\n",
       "      <td>0.485291</td>\n",
       "      <td>-0.504657</td>\n",
       "      <td>0.497366</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2006-12-04</th>\n",
       "      <td>-0.251553</td>\n",
       "      <td>-0.230349</td>\n",
       "      <td>-0.253761</td>\n",
       "      <td>-0.230211</td>\n",
       "      <td>0.103131</td>\n",
       "      <td>-0.535039</td>\n",
       "      <td>0.485291</td>\n",
       "      <td>-0.404657</td>\n",
       "      <td>-0.502634</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2006-12-05</th>\n",
       "      <td>-0.235158</td>\n",
       "      <td>-0.229285</td>\n",
       "      <td>-0.238149</td>\n",
       "      <td>-0.233408</td>\n",
       "      <td>0.046997</td>\n",
       "      <td>-0.535039</td>\n",
       "      <td>0.485291</td>\n",
       "      <td>-0.371323</td>\n",
       "      <td>-0.252634</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2006-12-06</th>\n",
       "      <td>-0.238437</td>\n",
       "      <td>-0.236734</td>\n",
       "      <td>-0.249301</td>\n",
       "      <td>-0.237670</td>\n",
       "      <td>0.045932</td>\n",
       "      <td>-0.535039</td>\n",
       "      <td>0.485291</td>\n",
       "      <td>-0.337990</td>\n",
       "      <td>-0.002634</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2006-12-07</th>\n",
       "      <td>-0.242809</td>\n",
       "      <td>-0.232477</td>\n",
       "      <td>-0.243725</td>\n",
       "      <td>-0.237670</td>\n",
       "      <td>0.074448</td>\n",
       "      <td>-0.535039</td>\n",
       "      <td>0.485291</td>\n",
       "      <td>-0.304657</td>\n",
       "      <td>0.247366</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                open      high       low     close    volume      year  \\\n",
       "date                                                                     \n",
       "2006-10-27 -0.290901 -0.286750 -0.305056 -0.297344  0.947017 -0.535039   \n",
       "2006-10-30 -0.305110 -0.299520 -0.306172 -0.296279  0.080294 -0.535039   \n",
       "2006-10-31 -0.304017 -0.298456 -0.302826 -0.295213  0.032970 -0.535039   \n",
       "2006-11-01 -0.301831 -0.300584 -0.302826 -0.295213 -0.004811 -0.535039   \n",
       "2006-11-02 -0.301831 -0.301648 -0.306172 -0.297344  0.011611 -0.535039   \n",
       "2006-11-03 -0.305110 -0.294199 -0.303941 -0.288819  0.098982 -0.535039   \n",
       "2006-11-06 -0.298552 -0.293135 -0.298366 -0.286688  0.025147 -0.535039   \n",
       "2006-11-07 -0.293087 -0.287814 -0.292790 -0.284557  0.058874 -0.535039   \n",
       "2006-11-08 -0.290901 -0.291007 -0.295020 -0.288819  0.002545 -0.535039   \n",
       "2006-11-09 -0.295273 -0.291007 -0.295020 -0.284557  0.012696 -0.535039   \n",
       "2006-11-10 -0.290901 -0.278237 -0.289445 -0.278163  0.238736 -0.535039   \n",
       "2006-11-13 -0.285436 -0.270787 -0.284984 -0.271770  0.123600 -0.535039   \n",
       "2006-11-14 -0.271227 -0.267595 -0.274948 -0.263245  0.111375 -0.535039   \n",
       "2006-11-15 -0.267948 -0.264402 -0.267142 -0.257917  0.077452 -0.535039   \n",
       "2006-11-16 -0.258111 -0.234606 -0.259337 -0.241932  0.210768 -0.535039   \n",
       "2006-11-17 -0.248274 -0.243119 -0.251531 -0.241932  0.065887 -0.535039   \n",
       "2006-11-20 -0.247181 -0.235670 -0.244840 -0.229145  0.067133 -0.535039   \n",
       "2006-11-21 -0.235158 -0.231413 -0.238149 -0.228080  0.034110 -0.535039   \n",
       "2006-11-22 -0.235158 -0.232477 -0.243725 -0.231276  0.053692 -0.535039   \n",
       "2006-11-23 -0.234065 -0.226092 -0.231459 -0.223817  0.061264 -0.535039   \n",
       "2006-11-24 -0.234065 -0.233541 -0.238149 -0.229145  0.017078 -0.535039   \n",
       "2006-11-27 -0.237344 -0.237798 -0.249301 -0.240867  0.060302 -0.535039   \n",
       "2006-11-28 -0.249367 -0.247376 -0.254876 -0.247261  0.037753 -0.535039   \n",
       "2006-11-29 -0.261390 -0.248440 -0.261567 -0.242998  0.038482 -0.535039   \n",
       "2006-11-30 -0.247181 -0.245247 -0.248185 -0.240867  0.033632 -0.535039   \n",
       "2006-12-01 -0.243902 -0.245247 -0.251531 -0.246195  0.050096 -0.535039   \n",
       "2006-12-04 -0.251553 -0.230349 -0.253761 -0.230211  0.103131 -0.535039   \n",
       "2006-12-05 -0.235158 -0.229285 -0.238149 -0.233408  0.046997 -0.535039   \n",
       "2006-12-06 -0.238437 -0.236734 -0.249301 -0.237670  0.045932 -0.535039   \n",
       "2006-12-07 -0.242809 -0.232477 -0.243725 -0.237670  0.074448 -0.535039   \n",
       "\n",
       "               month      date       day  \n",
       "date                                      \n",
       "2006-10-27  0.303473  0.362010  0.497366  \n",
       "2006-10-30  0.303473  0.462010 -0.502634  \n",
       "2006-10-31  0.303473  0.495343 -0.252634  \n",
       "2006-11-01  0.394382 -0.504657 -0.002634  \n",
       "2006-11-02  0.394382 -0.471323  0.247366  \n",
       "2006-11-03  0.394382 -0.437990  0.497366  \n",
       "2006-11-06  0.394382 -0.337990 -0.502634  \n",
       "2006-11-07  0.394382 -0.304657 -0.252634  \n",
       "2006-11-08  0.394382 -0.271323 -0.002634  \n",
       "2006-11-09  0.394382 -0.237990  0.247366  \n",
       "2006-11-10  0.394382 -0.204657  0.497366  \n",
       "2006-11-13  0.394382 -0.104657 -0.502634  \n",
       "2006-11-14  0.394382 -0.071323 -0.252634  \n",
       "2006-11-15  0.394382 -0.037990 -0.002634  \n",
       "2006-11-16  0.394382 -0.004657  0.247366  \n",
       "2006-11-17  0.394382  0.028677  0.497366  \n",
       "2006-11-20  0.394382  0.128677 -0.502634  \n",
       "2006-11-21  0.394382  0.162010 -0.252634  \n",
       "2006-11-22  0.394382  0.195343 -0.002634  \n",
       "2006-11-23  0.394382  0.228677  0.247366  \n",
       "2006-11-24  0.394382  0.262010  0.497366  \n",
       "2006-11-27  0.394382  0.362010 -0.502634  \n",
       "2006-11-28  0.394382  0.395343 -0.252634  \n",
       "2006-11-29  0.394382  0.428677 -0.002634  \n",
       "2006-11-30  0.394382  0.462010  0.247366  \n",
       "2006-12-01  0.485291 -0.504657  0.497366  \n",
       "2006-12-04  0.485291 -0.404657 -0.502634  \n",
       "2006-12-05  0.485291 -0.371323 -0.252634  \n",
       "2006-12-06  0.485291 -0.337990 -0.002634  \n",
       "2006-12-07  0.485291 -0.304657  0.247366  "
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "array([[-0.290901  , -0.28674996, -0.30505644, -0.2973442 ,  0.94701728,\n",
       "        -0.53503852,  0.30347321,  0.36200997,  0.49736574],\n",
       "       [-0.30510993, -0.29952003, -0.30617156, -0.29627859,  0.08029418,\n",
       "        -0.53503852,  0.30347321,  0.46200997, -0.50263426],\n",
       "       [-0.30401693, -0.29845586, -0.30282621, -0.29521298,  0.03296992,\n",
       "        -0.53503852,  0.30347321,  0.4953433 , -0.25263426],\n",
       "       [-0.30183094, -0.3005842 , -0.30282621, -0.29521298, -0.00481098,\n",
       "        -0.53503852,  0.3943823 , -0.5046567 , -0.00263426],\n",
       "       [-0.30183094, -0.30164837, -0.30617156, -0.2973442 ,  0.01161062,\n",
       "        -0.53503852,  0.3943823 , -0.47132336,  0.24736574],\n",
       "       [-0.30510993, -0.29419917, -0.30394132, -0.28881931,  0.09898188,\n",
       "        -0.53503852,  0.3943823 , -0.43799003,  0.49736574],\n",
       "       [-0.29855196, -0.293135  , -0.29836574, -0.28668809,  0.02514733,\n",
       "        -0.53503852,  0.3943823 , -0.33799003, -0.50263426],\n",
       "       [-0.29308699, -0.28781414, -0.29279015, -0.28455687,  0.05887356,\n",
       "        -0.53503852,  0.3943823 , -0.3046567 , -0.25263426],\n",
       "       [-0.290901  , -0.29100665, -0.29502039, -0.28881931,  0.00254453,\n",
       "        -0.53503852,  0.3943823 , -0.27132336, -0.00263426],\n",
       "       [-0.29527298, -0.29100665, -0.29502039, -0.28455687,  0.01269578,\n",
       "        -0.53503852,  0.3943823 , -0.23799003,  0.24736574],\n",
       "       [-0.290901  , -0.27823659, -0.2894448 , -0.27816321,  0.23873587,\n",
       "        -0.53503852,  0.3943823 , -0.2046567 ,  0.49736574],\n",
       "       [-0.28543603, -0.27078738, -0.28498433, -0.27176955,  0.12360048,\n",
       "        -0.53503852,  0.3943823 , -0.1046567 , -0.50263426],\n",
       "       [-0.2712271 , -0.26759487, -0.27494828, -0.26324467,  0.11137515,\n",
       "        -0.53503852,  0.3943823 , -0.07132336, -0.25263426],\n",
       "       [-0.26794812, -0.26440235, -0.26714246, -0.25791662,  0.07745172,\n",
       "        -0.53503852,  0.3943823 , -0.03799003, -0.00263426],\n",
       "       [-0.25811117, -0.23460553, -0.25933663, -0.24193246,  0.21076845,\n",
       "        -0.53503852,  0.3943823 , -0.0046567 ,  0.24736574],\n",
       "       [-0.24827422, -0.24311891, -0.25153081, -0.24193246,  0.06588674,\n",
       "        -0.53503852,  0.3943823 ,  0.02867664,  0.49736574],\n",
       "       [-0.24718123, -0.2356697 , -0.24484011, -0.22914514,  0.06713312,\n",
       "        -0.53503852,  0.3943823 ,  0.12867664, -0.50263426],\n",
       "       [-0.23515829, -0.23141301, -0.23814941, -0.22807953,  0.03410991,\n",
       "        -0.53503852,  0.3943823 ,  0.16200997, -0.25263426],\n",
       "       [-0.23515829, -0.23247718, -0.24372499, -0.23127636,  0.05369203,\n",
       "        -0.53503852,  0.3943823 ,  0.1953433 , -0.00263426],\n",
       "       [-0.2340653 , -0.22609215, -0.2314587 , -0.22381709,  0.06126361,\n",
       "        -0.53503852,  0.3943823 ,  0.22867664,  0.24736574],\n",
       "       [-0.2340653 , -0.23354136, -0.23814941, -0.22914514,  0.01707835,\n",
       "        -0.53503852,  0.3943823 ,  0.26200997,  0.49736574],\n",
       "       [-0.23734428, -0.23779804, -0.24930058, -0.24086685,  0.06030158,\n",
       "        -0.53503852,  0.3943823 ,  0.36200997, -0.50263426],\n",
       "       [-0.24936722, -0.24737559, -0.25487617, -0.24726051,  0.0377531 ,\n",
       "        -0.53503852,  0.3943823 ,  0.3953433 , -0.25263426],\n",
       "       [-0.26139015, -0.24843977, -0.26156687, -0.24299807,  0.03848157,\n",
       "        -0.53503852,  0.3943823 ,  0.42867664, -0.00263426],\n",
       "       [-0.24718123, -0.24524725, -0.24818546, -0.24086685,  0.03363159,\n",
       "        -0.53503852,  0.3943823 ,  0.46200997,  0.24736574],\n",
       "       [-0.24390224, -0.24524725, -0.25153081, -0.2461949 ,  0.05009644,\n",
       "        -0.53503852,  0.48529139, -0.5046567 ,  0.49736574],\n",
       "       [-0.2515532 , -0.23034884, -0.25376105, -0.23021075,  0.10313082,\n",
       "        -0.53503852,  0.48529139, -0.4046567 , -0.50263426],\n",
       "       [-0.23515829, -0.22928467, -0.23814941, -0.23340758,  0.04699657,\n",
       "        -0.53503852,  0.48529139, -0.37132336, -0.25263426],\n",
       "       [-0.23843727, -0.23673387, -0.24930058, -0.23767002,  0.04593161,\n",
       "        -0.53503852,  0.48529139, -0.33799003, -0.00263426],\n",
       "       [-0.24280925, -0.23247718, -0.24372499, -0.23767002,  0.07444752,\n",
       "        -0.53503852,  0.48529139, -0.3046567 ,  0.24736574]])"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#X_train集合中每一个元素（30天的数据）与X集合中每一个元素的对应关系（因为是按照顺序拆分的）\n",
    "X_train[0]\n",
    "np.array([np.array(x.values) for x in X])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((2325, 30, 9), (582, 30, 9), (2325, 5), (582, 5))"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#将原本的集合中包含DataFrame的X，或者集合中包含Series的Y，转换成np.array类型\n",
    "X_train=toNpArray(X_train)\n",
    "X_test=toNpArray(X_test)\n",
    "y_train=toNpArray(y_train)\n",
    "y_test=toNpArray(y_test)\n",
    "X_train.shape,X_test.shape,y_train.shape,y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://keras.io/zh/layers/recurrent/#lstm\n",
    "    \n",
    "[Sequential 顺序模型指引](https://keras.io/zh/getting-started/sequential-model-guide/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "def buildManyToManyModel(shape):\n",
    "    model = Sequential()\n",
    "    #input_dim: 输入的维度（整数）。 将此层用作模型中的第一层时，此参数（或者，关键字参数 input_shape）是必需的。\n",
    "    #input_length: 输入序列的长度，在恒定时指定。 \n",
    "    #如果你要在上游连接 Flatten 和 Dense 层， 则需要此参数（如果没有它，无法计算全连接输出的尺寸）。 \n",
    "    #请注意，如果循环神经网络层不是模型中的第一层， 则需要在第一层的层级指定输入长度（例如，通过 input_shape 参数）。\n",
    "    model.add(LSTM(10, input_length=shape[1], input_dim=shape[2]))\n",
    "    # \n",
    "    model.add(Dense(5))\n",
    "    # 模型编译\n",
    "    # 在训练模型之前，您需要配置学习过程，这是通过 compile 方法完成的。它接收三个参数：\n",
    "\n",
    "    # 优化器 optimizer。它可以是现有优化器的字符串标识符，如 rmsprop 或 adagrad，也可以是 Optimizer 类的实例。\n",
    "    # 详见：optimizers。\n",
    "    # 损失函数 loss，模型试图最小化的目标函数。它可以是现有损失函数的字符串标识符，\n",
    "    # 如 categorical_crossentropy 或 mse，也可以是一个目标函数。详见：losses。\n",
    "    # 评估标准 metrics。对于任何分类问题，你都希望将其设置为 metrics = ['accuracy']。\n",
    "    # 评估标准可以是现有的标准的字符串标识符，也可以是自定义的评估标准函数。\n",
    "    model.compile(loss=\"mse\", optimizer=\"adam\")\n",
    "    model.summary()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_9 (LSTM)                (None, 10)                800       \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 5)                 55        \n",
      "=================================================================\n",
      "Total params: 855\n",
      "Trainable params: 855\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 2470 samples, validate on 437 samples\n",
      "Epoch 1/1000\n",
      "2470/2470 [==============================] - 2s 885us/step - loss: 0.0275 - val_loss: 0.2189\n",
      "Epoch 2/1000\n",
      "2470/2470 [==============================] - 1s 277us/step - loss: 0.0137 - val_loss: 0.1485\n",
      "Epoch 3/1000\n",
      "2470/2470 [==============================] - 1s 304us/step - loss: 0.0073 - val_loss: 0.0872\n",
      "Epoch 4/1000\n",
      "2470/2470 [==============================] - 1s 322us/step - loss: 0.0036 - val_loss: 0.0512\n",
      "Epoch 5/1000\n",
      "2470/2470 [==============================] - 1s 306us/step - loss: 0.0021 - val_loss: 0.0390\n",
      "Epoch 6/1000\n",
      "2470/2470 [==============================] - 1s 310us/step - loss: 0.0016 - val_loss: 0.0380\n",
      "Epoch 7/1000\n",
      "2470/2470 [==============================] - 1s 312us/step - loss: 0.0013 - val_loss: 0.0373\n",
      "Epoch 8/1000\n",
      "2470/2470 [==============================] - 1s 341us/step - loss: 0.0011 - val_loss: 0.0363\n",
      "Epoch 9/1000\n",
      "2470/2470 [==============================] - 1s 291us/step - loss: 0.0010 - val_loss: 0.0356\n",
      "Epoch 10/1000\n",
      "2470/2470 [==============================] - 1s 303us/step - loss: 9.2911e-04 - val_loss: 0.0354\n",
      "Epoch 11/1000\n",
      "2470/2470 [==============================] - 1s 330us/step - loss: 8.6721e-04 - val_loss: 0.0348\n",
      "Epoch 12/1000\n",
      "2470/2470 [==============================] - 1s 322us/step - loss: 8.1726e-04 - val_loss: 0.0346\n",
      "Epoch 13/1000\n",
      "2470/2470 [==============================] - 1s 334us/step - loss: 7.7848e-04 - val_loss: 0.0342\n",
      "Epoch 14/1000\n",
      "2470/2470 [==============================] - 1s 324us/step - loss: 7.4737e-04 - val_loss: 0.0340\n",
      "Epoch 15/1000\n",
      "2470/2470 [==============================] - 1s 323us/step - loss: 7.2409e-04 - val_loss: 0.0331\n",
      "Epoch 16/1000\n",
      "2470/2470 [==============================] - 1s 330us/step - loss: 7.0099e-04 - val_loss: 0.0336\n",
      "Epoch 17/1000\n",
      "2470/2470 [==============================] - 1s 314us/step - loss: 6.8357e-04 - val_loss: 0.0330\n",
      "Epoch 18/1000\n",
      "2470/2470 [==============================] - 1s 293us/step - loss: 6.6948e-04 - val_loss: 0.0327\n",
      "Epoch 19/1000\n",
      "2470/2470 [==============================] - 1s 311us/step - loss: 6.5610e-04 - val_loss: 0.0325\n",
      "Epoch 20/1000\n",
      "2470/2470 [==============================] - 1s 297us/step - loss: 6.4520e-04 - val_loss: 0.0318\n",
      "Epoch 21/1000\n",
      "2470/2470 [==============================] - 1s 331us/step - loss: 6.3480e-04 - val_loss: 0.0317\n",
      "Epoch 22/1000\n",
      "2470/2470 [==============================] - 1s 322us/step - loss: 6.2526e-04 - val_loss: 0.0318\n",
      "Epoch 23/1000\n",
      "2470/2470 [==============================] - 1s 318us/step - loss: 6.1678e-04 - val_loss: 0.0311\n",
      "Epoch 24/1000\n",
      "2470/2470 [==============================] - 1s 330us/step - loss: 6.0863e-04 - val_loss: 0.0308\n",
      "Epoch 25/1000\n",
      "2470/2470 [==============================] - 1s 328us/step - loss: 6.0264e-04 - val_loss: 0.0303\n",
      "Epoch 26/1000\n",
      "2470/2470 [==============================] - 1s 330us/step - loss: 5.9592e-04 - val_loss: 0.0307\n",
      "Epoch 27/1000\n",
      "2470/2470 [==============================] - 1s 316us/step - loss: 5.9194e-04 - val_loss: 0.0300\n",
      "Epoch 28/1000\n",
      "2470/2470 [==============================] - 1s 324us/step - loss: 5.8522e-04 - val_loss: 0.0297\n",
      "Epoch 29/1000\n",
      "2470/2470 [==============================] - 1s 333us/step - loss: 5.8126e-04 - val_loss: 0.0293\n",
      "Epoch 30/1000\n",
      "2470/2470 [==============================] - 1s 325us/step - loss: 5.7254e-04 - val_loss: 0.0290\n",
      "Epoch 31/1000\n",
      "2470/2470 [==============================] - 1s 345us/step - loss: 5.6838e-04 - val_loss: 0.0289\n",
      "Epoch 32/1000\n",
      "2470/2470 [==============================] - 1s 312us/step - loss: 5.6463e-04 - val_loss: 0.0282\n",
      "Epoch 33/1000\n",
      "2470/2470 [==============================] - 1s 314us/step - loss: 5.6031e-04 - val_loss: 0.0280\n",
      "Epoch 34/1000\n",
      "2470/2470 [==============================] - 1s 329us/step - loss: 5.5863e-04 - val_loss: 0.0281\n",
      "Epoch 35/1000\n",
      "2470/2470 [==============================] - 1s 318us/step - loss: 5.5192e-04 - val_loss: 0.0273\n",
      "Epoch 36/1000\n",
      "2470/2470 [==============================] - 1s 320us/step - loss: 5.4838e-04 - val_loss: 0.0271\n",
      "Epoch 37/1000\n",
      "2470/2470 [==============================] - 1s 332us/step - loss: 5.4391e-04 - val_loss: 0.0271\n",
      "Epoch 38/1000\n",
      "2470/2470 [==============================] - 1s 284us/step - loss: 5.4062e-04 - val_loss: 0.0264\n",
      "Epoch 39/1000\n",
      "2470/2470 [==============================] - 1s 303us/step - loss: 5.3986e-04 - val_loss: 0.0265\n",
      "Epoch 40/1000\n",
      "2470/2470 [==============================] - 1s 321us/step - loss: 5.3605e-04 - val_loss: 0.0263\n",
      "Epoch 41/1000\n",
      "2470/2470 [==============================] - 1s 320us/step - loss: 5.3341e-04 - val_loss: 0.0258\n",
      "Epoch 42/1000\n",
      "2470/2470 [==============================] - 1s 314us/step - loss: 5.2831e-04 - val_loss: 0.0251\n",
      "Epoch 43/1000\n",
      "2470/2470 [==============================] - 1s 325us/step - loss: 5.2823e-04 - val_loss: 0.0251\n",
      "Epoch 44/1000\n",
      "2470/2470 [==============================] - 1s 295us/step - loss: 5.2017e-04 - val_loss: 0.0247\n",
      "Epoch 45/1000\n",
      "2470/2470 [==============================] - 1s 331us/step - loss: 5.2065e-04 - val_loss: 0.0249\n",
      "Epoch 46/1000\n",
      "2470/2470 [==============================] - 1s 291us/step - loss: 5.1741e-04 - val_loss: 0.0242\n",
      "Epoch 47/1000\n",
      "2470/2470 [==============================] - 1s 302us/step - loss: 5.1428e-04 - val_loss: 0.0240\n",
      "Epoch 48/1000\n",
      "2470/2470 [==============================] - 1s 322us/step - loss: 5.1073e-04 - val_loss: 0.0234\n",
      "Epoch 49/1000\n",
      "2470/2470 [==============================] - 1s 326us/step - loss: 5.0924e-04 - val_loss: 0.0232\n",
      "Epoch 50/1000\n",
      "2470/2470 [==============================] - 1s 287us/step - loss: 5.0569e-04 - val_loss: 0.0230\n",
      "Epoch 51/1000\n",
      "2470/2470 [==============================] - 1s 275us/step - loss: 5.0359e-04 - val_loss: 0.0227\n",
      "Epoch 52/1000\n",
      "2470/2470 [==============================] - 1s 373us/step - loss: 5.0102e-04 - val_loss: 0.0221\n",
      "Epoch 53/1000\n",
      "2470/2470 [==============================] - 1s 354us/step - loss: 4.9966e-04 - val_loss: 0.0223\n",
      "Epoch 54/1000\n",
      "2470/2470 [==============================] - 1s 328us/step - loss: 4.9672e-04 - val_loss: 0.0222\n",
      "Epoch 55/1000\n",
      "2470/2470 [==============================] - 1s 334us/step - loss: 4.9538e-04 - val_loss: 0.0218\n",
      "Epoch 56/1000\n",
      "2470/2470 [==============================] - 1s 314us/step - loss: 4.9298e-04 - val_loss: 0.0218\n",
      "Epoch 57/1000\n",
      "2470/2470 [==============================] - 1s 300us/step - loss: 4.9123e-04 - val_loss: 0.0211\n",
      "Epoch 58/1000\n",
      "2470/2470 [==============================] - 1s 324us/step - loss: 4.8894e-04 - val_loss: 0.0213\n",
      "Epoch 59/1000\n",
      "2470/2470 [==============================] - 1s 372us/step - loss: 4.8789e-04 - val_loss: 0.0207\n",
      "Epoch 60/1000\n",
      "2470/2470 [==============================] - 1s 320us/step - loss: 4.8592e-04 - val_loss: 0.0207\n",
      "Epoch 61/1000\n",
      "2470/2470 [==============================] - 1s 327us/step - loss: 4.8454e-04 - val_loss: 0.0204\n",
      "Epoch 62/1000\n",
      "2470/2470 [==============================] - 1s 322us/step - loss: 4.8219e-04 - val_loss: 0.0205\n",
      "Epoch 63/1000\n",
      "2470/2470 [==============================] - 1s 315us/step - loss: 4.8020e-04 - val_loss: 0.0199\n",
      "Epoch 64/1000\n",
      "2470/2470 [==============================] - 1s 352us/step - loss: 4.7921e-04 - val_loss: 0.0199\n",
      "Epoch 65/1000\n",
      "2470/2470 [==============================] - 1s 319us/step - loss: 4.7625e-04 - val_loss: 0.0195\n",
      "Epoch 66/1000\n",
      "2470/2470 [==============================] - 1s 311us/step - loss: 4.7821e-04 - val_loss: 0.0196\n",
      "Epoch 67/1000\n",
      "2470/2470 [==============================] - 1s 319us/step - loss: 4.7675e-04 - val_loss: 0.0193\n",
      "Epoch 68/1000\n",
      "2470/2470 [==============================] - 1s 275us/step - loss: 4.7383e-04 - val_loss: 0.0189\n",
      "Epoch 69/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2470/2470 [==============================] - 1s 318us/step - loss: 4.7021e-04 - val_loss: 0.0185\n",
      "Epoch 70/1000\n",
      "2470/2470 [==============================] - 1s 325us/step - loss: 4.7033e-04 - val_loss: 0.0185\n",
      "Epoch 71/1000\n",
      "2470/2470 [==============================] - 1s 313us/step - loss: 4.6730e-04 - val_loss: 0.0188\n",
      "Epoch 72/1000\n",
      "2470/2470 [==============================] - 1s 302us/step - loss: 4.6589e-04 - val_loss: 0.0184\n",
      "Epoch 73/1000\n",
      "2470/2470 [==============================] - 1s 321us/step - loss: 4.6424e-04 - val_loss: 0.0181\n",
      "Epoch 74/1000\n",
      "2470/2470 [==============================] - 1s 332us/step - loss: 4.7179e-04 - val_loss: 0.0182\n",
      "Epoch 75/1000\n",
      "2470/2470 [==============================] - 1s 328us/step - loss: 4.6474e-04 - val_loss: 0.0180\n",
      "Epoch 76/1000\n",
      "2470/2470 [==============================] - 1s 269us/step - loss: 4.6197e-04 - val_loss: 0.0176\n",
      "Epoch 77/1000\n",
      "2470/2470 [==============================] - 1s 294us/step - loss: 4.5910e-04 - val_loss: 0.0175\n",
      "Epoch 78/1000\n",
      "2470/2470 [==============================] - 1s 327us/step - loss: 4.5883e-04 - val_loss: 0.0175\n",
      "Epoch 79/1000\n",
      "2470/2470 [==============================] - 1s 326us/step - loss: 4.6034e-04 - val_loss: 0.0172\n",
      "Epoch 80/1000\n",
      "2470/2470 [==============================] - 1s 308us/step - loss: 4.5503e-04 - val_loss: 0.0168\n",
      "Epoch 81/1000\n",
      "2470/2470 [==============================] - 1s 300us/step - loss: 4.5328e-04 - val_loss: 0.0167\n",
      "Epoch 82/1000\n",
      "2470/2470 [==============================] - 1s 335us/step - loss: 4.5328e-04 - val_loss: 0.0167\n",
      "Epoch 83/1000\n",
      "2470/2470 [==============================] - 1s 362us/step - loss: 4.5157e-04 - val_loss: 0.0166\n",
      "Epoch 84/1000\n",
      "2470/2470 [==============================] - 1s 343us/step - loss: 4.5058e-04 - val_loss: 0.0163\n",
      "Epoch 85/1000\n",
      "2470/2470 [==============================] - 1s 348us/step - loss: 4.4995e-04 - val_loss: 0.0161\n",
      "Epoch 86/1000\n",
      "2470/2470 [==============================] - 1s 328us/step - loss: 4.5055e-04 - val_loss: 0.0160\n",
      "Epoch 87/1000\n",
      "2470/2470 [==============================] - 1s 297us/step - loss: 4.4584e-04 - val_loss: 0.0161\n",
      "Epoch 88/1000\n",
      "2470/2470 [==============================] - 1s 304us/step - loss: 4.4738e-04 - val_loss: 0.0154\n",
      "Epoch 89/1000\n",
      "2470/2470 [==============================] - 1s 293us/step - loss: 4.4517e-04 - val_loss: 0.0156\n",
      "Epoch 90/1000\n",
      "2470/2470 [==============================] - 1s 302us/step - loss: 4.4297e-04 - val_loss: 0.0154\n",
      "Epoch 91/1000\n",
      "2470/2470 [==============================] - 1s 337us/step - loss: 4.4241e-04 - val_loss: 0.0153\n",
      "Epoch 92/1000\n",
      "2470/2470 [==============================] - 1s 325us/step - loss: 4.4168e-04 - val_loss: 0.0153\n",
      "Epoch 93/1000\n",
      "2470/2470 [==============================] - 1s 318us/step - loss: 4.4184e-04 - val_loss: 0.0151\n",
      "Epoch 94/1000\n",
      "2470/2470 [==============================] - 1s 323us/step - loss: 4.4006e-04 - val_loss: 0.0148\n",
      "Epoch 95/1000\n",
      "2470/2470 [==============================] - 1s 320us/step - loss: 4.3889e-04 - val_loss: 0.0147\n",
      "Epoch 96/1000\n",
      "2470/2470 [==============================] - 1s 326us/step - loss: 4.3723e-04 - val_loss: 0.0146\n",
      "Epoch 97/1000\n",
      "2470/2470 [==============================] - 1s 309us/step - loss: 4.3558e-04 - val_loss: 0.0148\n",
      "Epoch 98/1000\n",
      "2470/2470 [==============================] - 1s 320us/step - loss: 4.3524e-04 - val_loss: 0.0140\n",
      "Epoch 99/1000\n",
      "2470/2470 [==============================] - 1s 282us/step - loss: 4.3582e-04 - val_loss: 0.0141\n",
      "Epoch 100/1000\n",
      "2470/2470 [==============================] - 1s 298us/step - loss: 4.3520e-04 - val_loss: 0.0140\n",
      "Epoch 101/1000\n",
      "2470/2470 [==============================] - 1s 321us/step - loss: 4.3467e-04 - val_loss: 0.0141\n",
      "Epoch 102/1000\n",
      "2470/2470 [==============================] - 1s 322us/step - loss: 4.3240e-04 - val_loss: 0.0138\n",
      "Epoch 103/1000\n",
      "2470/2470 [==============================] - 1s 333us/step - loss: 4.3114e-04 - val_loss: 0.0139\n",
      "Epoch 104/1000\n",
      "2470/2470 [==============================] - 1s 311us/step - loss: 4.2965e-04 - val_loss: 0.0135\n",
      "Epoch 105/1000\n",
      "2470/2470 [==============================] - 1s 326us/step - loss: 4.2885e-04 - val_loss: 0.0136\n",
      "Epoch 106/1000\n",
      "2470/2470 [==============================] - 1s 312us/step - loss: 4.2772e-04 - val_loss: 0.0135\n",
      "Epoch 107/1000\n",
      "2470/2470 [==============================] - 1s 306us/step - loss: 4.2726e-04 - val_loss: 0.0134\n",
      "Epoch 108/1000\n",
      "2470/2470 [==============================] - 1s 339us/step - loss: 4.2756e-04 - val_loss: 0.0132\n",
      "Epoch 109/1000\n",
      "2470/2470 [==============================] - 1s 283us/step - loss: 4.2665e-04 - val_loss: 0.0131\n",
      "Epoch 110/1000\n",
      "2470/2470 [==============================] - 1s 314us/step - loss: 4.2442e-04 - val_loss: 0.0128\n",
      "Epoch 111/1000\n",
      "2470/2470 [==============================] - 1s 285us/step - loss: 4.2583e-04 - val_loss: 0.0129\n",
      "Epoch 112/1000\n",
      "2470/2470 [==============================] - 1s 318us/step - loss: 4.2537e-04 - val_loss: 0.0131\n",
      "Epoch 113/1000\n",
      "2470/2470 [==============================] - 1s 331us/step - loss: 4.2635e-04 - val_loss: 0.0128\n",
      "Epoch 114/1000\n",
      "2470/2470 [==============================] - 1s 277us/step - loss: 4.2086e-04 - val_loss: 0.0129\n",
      "Epoch 115/1000\n",
      "2470/2470 [==============================] - 1s 299us/step - loss: 4.2195e-04 - val_loss: 0.0123\n",
      "Epoch 116/1000\n",
      "2470/2470 [==============================] - 1s 326us/step - loss: 4.2098e-04 - val_loss: 0.0127\n",
      "Epoch 117/1000\n",
      "2470/2470 [==============================] - 1s 325us/step - loss: 4.1923e-04 - val_loss: 0.0123\n",
      "Epoch 118/1000\n",
      "2470/2470 [==============================] - 1s 282us/step - loss: 4.2015e-04 - val_loss: 0.0123\n",
      "Epoch 119/1000\n",
      "2470/2470 [==============================] - 1s 327us/step - loss: 4.1835e-04 - val_loss: 0.0120\n",
      "Epoch 120/1000\n",
      "2470/2470 [==============================] - 1s 327us/step - loss: 4.1891e-04 - val_loss: 0.0116\n",
      "Epoch 121/1000\n",
      "2470/2470 [==============================] - 1s 308us/step - loss: 4.1755e-04 - val_loss: 0.0118\n",
      "Epoch 122/1000\n",
      "2470/2470 [==============================] - 1s 324us/step - loss: 4.1520e-04 - val_loss: 0.0118\n",
      "Epoch 123/1000\n",
      "2470/2470 [==============================] - 1s 269us/step - loss: 4.1666e-04 - val_loss: 0.0119\n",
      "Epoch 124/1000\n",
      "2470/2470 [==============================] - 1s 274us/step - loss: 4.1547e-04 - val_loss: 0.0115\n",
      "Epoch 125/1000\n",
      "2470/2470 [==============================] - 1s 263us/step - loss: 4.1426e-04 - val_loss: 0.0116\n",
      "Epoch 126/1000\n",
      "2470/2470 [==============================] - 1s 298us/step - loss: 4.1168e-04 - val_loss: 0.0113\n",
      "Epoch 127/1000\n",
      "2470/2470 [==============================] - 1s 309us/step - loss: 4.1133e-04 - val_loss: 0.0113\n",
      "Epoch 128/1000\n",
      "2470/2470 [==============================] - 1s 335us/step - loss: 4.1032e-04 - val_loss: 0.0113\n",
      "Epoch 129/1000\n",
      "2470/2470 [==============================] - 1s 315us/step - loss: 4.0963e-04 - val_loss: 0.0115\n",
      "Epoch 130/1000\n",
      "2470/2470 [==============================] - 1s 305us/step - loss: 4.1139e-04 - val_loss: 0.0108\n",
      "Epoch 131/1000\n",
      "2470/2470 [==============================] - 1s 292us/step - loss: 4.1012e-04 - val_loss: 0.0110\n",
      "Epoch 132/1000\n",
      "2470/2470 [==============================] - 1s 290us/step - loss: 4.0975e-04 - val_loss: 0.0108\n",
      "Epoch 133/1000\n",
      "2470/2470 [==============================] - 1s 310us/step - loss: 4.0861e-04 - val_loss: 0.0111\n",
      "Epoch 134/1000\n",
      "2470/2470 [==============================] - 1s 314us/step - loss: 4.0848e-04 - val_loss: 0.0109\n",
      "Epoch 135/1000\n",
      "2470/2470 [==============================] - 1s 281us/step - loss: 4.0709e-04 - val_loss: 0.0109\n",
      "Epoch 136/1000\n",
      "2470/2470 [==============================] - 1s 306us/step - loss: 4.0636e-04 - val_loss: 0.0108\n",
      "Epoch 137/1000\n",
      "2470/2470 [==============================] - 1s 328us/step - loss: 4.0713e-04 - val_loss: 0.0105\n",
      "Epoch 138/1000\n",
      "2470/2470 [==============================] - 1s 322us/step - loss: 4.0653e-04 - val_loss: 0.0107\n",
      "Epoch 139/1000\n",
      "2470/2470 [==============================] - 1s 315us/step - loss: 4.0383e-04 - val_loss: 0.0103\n",
      "Epoch 140/1000\n",
      "2470/2470 [==============================] - 1s 297us/step - loss: 4.0351e-04 - val_loss: 0.0102\n",
      "Epoch 141/1000\n",
      "2470/2470 [==============================] - 1s 290us/step - loss: 4.0505e-04 - val_loss: 0.0103\n",
      "Epoch 142/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2470/2470 [==============================] - 1s 306us/step - loss: 4.0390e-04 - val_loss: 0.0101\n",
      "Epoch 143/1000\n",
      "2470/2470 [==============================] - 1s 315us/step - loss: 4.0148e-04 - val_loss: 0.0101\n",
      "Epoch 144/1000\n",
      "2470/2470 [==============================] - 1s 303us/step - loss: 4.0096e-04 - val_loss: 0.0102\n",
      "Epoch 145/1000\n",
      "2470/2470 [==============================] - 1s 306us/step - loss: 4.0120e-04 - val_loss: 0.0100\n",
      "Epoch 146/1000\n",
      "2470/2470 [==============================] - 1s 312us/step - loss: 4.0338e-04 - val_loss: 0.0106\n",
      "Epoch 147/1000\n",
      "2470/2470 [==============================] - 1s 308us/step - loss: 3.9946e-04 - val_loss: 0.0099\n",
      "Epoch 148/1000\n",
      "2470/2470 [==============================] - 1s 318us/step - loss: 3.9896e-04 - val_loss: 0.0099\n",
      "Epoch 149/1000\n",
      "2470/2470 [==============================] - 1s 316us/step - loss: 3.9854e-04 - val_loss: 0.0100\n",
      "Epoch 150/1000\n",
      "2470/2470 [==============================] - 1s 271us/step - loss: 3.9831e-04 - val_loss: 0.0095\n",
      "Epoch 151/1000\n",
      "2470/2470 [==============================] - 1s 267us/step - loss: 3.9731e-04 - val_loss: 0.0096\n",
      "Epoch 152/1000\n",
      "2470/2470 [==============================] - 1s 311us/step - loss: 3.9682e-04 - val_loss: 0.0096\n",
      "Epoch 153/1000\n",
      "2470/2470 [==============================] - 1s 322us/step - loss: 3.9529e-04 - val_loss: 0.0097\n",
      "Epoch 154/1000\n",
      "2470/2470 [==============================] - 1s 319us/step - loss: 3.9609e-04 - val_loss: 0.0096\n",
      "Epoch 155/1000\n",
      "2470/2470 [==============================] - 1s 297us/step - loss: 3.9556e-04 - val_loss: 0.0094\n",
      "Epoch 156/1000\n",
      "2470/2470 [==============================] - 1s 324us/step - loss: 3.9699e-04 - val_loss: 0.0095\n",
      "Epoch 157/1000\n",
      "2470/2470 [==============================] - 1s 300us/step - loss: 3.9468e-04 - val_loss: 0.0093\n",
      "Epoch 158/1000\n",
      "2470/2470 [==============================] - 1s 341us/step - loss: 3.9464e-04 - val_loss: 0.0096\n",
      "Epoch 159/1000\n",
      "2470/2470 [==============================] - 1s 361us/step - loss: 3.9320e-04 - val_loss: 0.0092\n",
      "Epoch 160/1000\n",
      "2470/2470 [==============================] - 1s 324us/step - loss: 3.9298e-04 - val_loss: 0.0094\n",
      "Epoch 161/1000\n",
      "2470/2470 [==============================] - 1s 508us/step - loss: 3.9110e-04 - val_loss: 0.0093\n",
      "Epoch 162/1000\n",
      "2470/2470 [==============================] - 1s 353us/step - loss: 3.9221e-04 - val_loss: 0.0096\n",
      "Epoch 163/1000\n",
      "2470/2470 [==============================] - 1s 560us/step - loss: 3.9068e-04 - val_loss: 0.0090\n",
      "Epoch 164/1000\n",
      "2470/2470 [==============================] - 1s 571us/step - loss: 3.9348e-04 - val_loss: 0.0091\n",
      "Epoch 165/1000\n",
      "2470/2470 [==============================] - 1s 557us/step - loss: 3.9210e-04 - val_loss: 0.0091\n",
      "Epoch 166/1000\n",
      "2470/2470 [==============================] - 1s 554us/step - loss: 3.9209e-04 - val_loss: 0.0087\n",
      "Epoch 167/1000\n",
      "2470/2470 [==============================] - 1s 416us/step - loss: 3.8813e-04 - val_loss: 0.0089\n",
      "Epoch 168/1000\n",
      "2470/2470 [==============================] - 1s 444us/step - loss: 3.8901e-04 - val_loss: 0.0090\n",
      "Epoch 169/1000\n",
      "2470/2470 [==============================] - 1s 353us/step - loss: 3.8941e-04 - val_loss: 0.0089\n",
      "Epoch 170/1000\n",
      "2470/2470 [==============================] - 1s 344us/step - loss: 3.8914e-04 - val_loss: 0.0086\n",
      "Epoch 171/1000\n",
      "2470/2470 [==============================] - 1s 274us/step - loss: 3.8755e-04 - val_loss: 0.0087\n",
      "Epoch 172/1000\n",
      "2470/2470 [==============================] - 1s 264us/step - loss: 3.8732e-04 - val_loss: 0.0086\n",
      "Epoch 173/1000\n",
      "2470/2470 [==============================] - 1s 251us/step - loss: 3.8540e-04 - val_loss: 0.0088\n",
      "Epoch 174/1000\n",
      "2470/2470 [==============================] - 1s 245us/step - loss: 3.8464e-04 - val_loss: 0.0083\n",
      "Epoch 175/1000\n",
      "2470/2470 [==============================] - 1s 313us/step - loss: 3.8611e-04 - val_loss: 0.0080\n",
      "Epoch 176/1000\n",
      "2470/2470 [==============================] - 1s 330us/step - loss: 3.8669e-04 - val_loss: 0.0080\n",
      "Epoch 177/1000\n",
      "2470/2470 [==============================] - 1s 316us/step - loss: 3.8483e-04 - val_loss: 0.0086\n",
      "Epoch 178/1000\n",
      "2470/2470 [==============================] - 1s 298us/step - loss: 3.8379e-04 - val_loss: 0.0080\n",
      "Epoch 179/1000\n",
      "2470/2470 [==============================] - 1s 319us/step - loss: 3.8455e-04 - val_loss: 0.0082\n",
      "Epoch 180/1000\n",
      "2470/2470 [==============================] - 1s 317us/step - loss: 3.8201e-04 - val_loss: 0.0082\n",
      "Epoch 181/1000\n",
      "2470/2470 [==============================] - 1s 333us/step - loss: 3.8611e-04 - val_loss: 0.0080\n",
      "Epoch 182/1000\n",
      "2470/2470 [==============================] - 1s 325us/step - loss: 3.8285e-04 - val_loss: 0.0083\n",
      "Epoch 183/1000\n",
      "2470/2470 [==============================] - 1s 312us/step - loss: 3.8126e-04 - val_loss: 0.0080\n",
      "Epoch 184/1000\n",
      "2470/2470 [==============================] - 1s 327us/step - loss: 3.8013e-04 - val_loss: 0.0080\n",
      "Epoch 185/1000\n",
      "2470/2470 [==============================] - 1s 306us/step - loss: 3.7952e-04 - val_loss: 0.0079\n",
      "Epoch 186/1000\n",
      "2470/2470 [==============================] - 1s 264us/step - loss: 3.7881e-04 - val_loss: 0.0080\n",
      "Epoch 187/1000\n",
      "2470/2470 [==============================] - 1s 275us/step - loss: 3.7858e-04 - val_loss: 0.0077\n",
      "Epoch 188/1000\n",
      "2470/2470 [==============================] - 1s 321us/step - loss: 3.7794e-04 - val_loss: 0.0080\n",
      "Epoch 189/1000\n",
      "2470/2470 [==============================] - 1s 324us/step - loss: 3.7924e-04 - val_loss: 0.0077\n",
      "Epoch 190/1000\n",
      "2470/2470 [==============================] - 1s 288us/step - loss: 3.7775e-04 - val_loss: 0.0079\n",
      "Epoch 191/1000\n",
      "2470/2470 [==============================] - 1s 295us/step - loss: 3.7805e-04 - val_loss: 0.0078\n",
      "Epoch 192/1000\n",
      "2470/2470 [==============================] - 1s 340us/step - loss: 3.7714e-04 - val_loss: 0.0075\n",
      "Epoch 193/1000\n",
      "2470/2470 [==============================] - 1s 325us/step - loss: 3.7673e-04 - val_loss: 0.0078\n",
      "Epoch 194/1000\n",
      "2470/2470 [==============================] - 1s 319us/step - loss: 3.7692e-04 - val_loss: 0.0076\n",
      "Epoch 195/1000\n",
      "2470/2470 [==============================] - 1s 290us/step - loss: 3.7736e-04 - val_loss: 0.0079\n",
      "Epoch 196/1000\n",
      "2470/2470 [==============================] - 1s 318us/step - loss: 3.7583e-04 - val_loss: 0.0076\n",
      "Epoch 197/1000\n",
      "2470/2470 [==============================] - 1s 307us/step - loss: 3.7803e-04 - val_loss: 0.0078\n",
      "Epoch 198/1000\n",
      "2470/2470 [==============================] - 1s 322us/step - loss: 3.7508e-04 - val_loss: 0.0076\n",
      "Epoch 199/1000\n",
      "2470/2470 [==============================] - 1s 320us/step - loss: 3.7483e-04 - val_loss: 0.0075\n",
      "Epoch 200/1000\n",
      "2470/2470 [==============================] - 1s 318us/step - loss: 3.7407e-04 - val_loss: 0.0072\n",
      "Epoch 201/1000\n",
      "2470/2470 [==============================] - 1s 333us/step - loss: 3.7511e-04 - val_loss: 0.0077\n",
      "Epoch 202/1000\n",
      "2470/2470 [==============================] - 1s 320us/step - loss: 3.7346e-04 - val_loss: 0.0075\n",
      "Epoch 203/1000\n",
      "2470/2470 [==============================] - 1s 321us/step - loss: 3.7178e-04 - val_loss: 0.0071\n",
      "Epoch 204/1000\n",
      "2470/2470 [==============================] - 1s 318us/step - loss: 3.7327e-04 - val_loss: 0.0079\n",
      "Epoch 205/1000\n",
      "2470/2470 [==============================] - 1s 306us/step - loss: 3.7174e-04 - val_loss: 0.0075\n",
      "Epoch 206/1000\n",
      "2470/2470 [==============================] - 1s 324us/step - loss: 3.7511e-04 - val_loss: 0.0073\n",
      "Epoch 207/1000\n",
      "2470/2470 [==============================] - 1s 313us/step - loss: 3.7209e-04 - val_loss: 0.0072\n",
      "Epoch 208/1000\n",
      "2470/2470 [==============================] - 1s 320us/step - loss: 3.7010e-04 - val_loss: 0.0073\n",
      "Epoch 209/1000\n",
      "2470/2470 [==============================] - 1s 317us/step - loss: 3.7001e-04 - val_loss: 0.0075\n",
      "Epoch 210/1000\n",
      "2470/2470 [==============================] - 1s 323us/step - loss: 3.7241e-04 - val_loss: 0.0072\n",
      "Epoch 211/1000\n",
      "2470/2470 [==============================] - 1s 319us/step - loss: 3.6983e-04 - val_loss: 0.0067\n",
      "Epoch 212/1000\n",
      "2470/2470 [==============================] - 1s 315us/step - loss: 3.7055e-04 - val_loss: 0.0073\n",
      "Epoch 213/1000\n",
      "2470/2470 [==============================] - 1s 316us/step - loss: 3.6987e-04 - val_loss: 0.0070\n",
      "Epoch 214/1000\n",
      "2470/2470 [==============================] - 1s 320us/step - loss: 3.7193e-04 - val_loss: 0.0072\n",
      "Epoch 215/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2470/2470 [==============================] - 1s 312us/step - loss: 3.6923e-04 - val_loss: 0.0071\n",
      "Epoch 216/1000\n",
      "2470/2470 [==============================] - 1s 320us/step - loss: 3.6639e-04 - val_loss: 0.0070\n",
      "Epoch 217/1000\n",
      "2470/2470 [==============================] - 1s 314us/step - loss: 3.6758e-04 - val_loss: 0.0071\n",
      "Epoch 218/1000\n",
      "2470/2470 [==============================] - 1s 295us/step - loss: 3.6615e-04 - val_loss: 0.0072\n",
      "Epoch 219/1000\n",
      "2470/2470 [==============================] - 1s 331us/step - loss: 3.6605e-04 - val_loss: 0.0069\n",
      "Epoch 220/1000\n",
      "2470/2470 [==============================] - 1s 307us/step - loss: 3.7404e-04 - val_loss: 0.0078\n",
      "Epoch 221/1000\n",
      "2470/2470 [==============================] - 1s 269us/step - loss: 3.6759e-04 - val_loss: 0.0066\n",
      "Epoch 222/1000\n",
      "2470/2470 [==============================] - 1s 320us/step - loss: 3.6520e-04 - val_loss: 0.0073\n",
      "Epoch 223/1000\n",
      "2470/2470 [==============================] - 1s 332us/step - loss: 3.6628e-04 - val_loss: 0.0071\n",
      "Epoch 224/1000\n",
      "2470/2470 [==============================] - 1s 291us/step - loss: 3.6798e-04 - val_loss: 0.0069\n",
      "Epoch 225/1000\n",
      "2470/2470 [==============================] - 1s 343us/step - loss: 3.6379e-04 - val_loss: 0.0066\n",
      "Epoch 226/1000\n",
      "2470/2470 [==============================] - 1s 330us/step - loss: 3.6474e-04 - val_loss: 0.0067\n",
      "Epoch 227/1000\n",
      "2470/2470 [==============================] - 1s 323us/step - loss: 3.6475e-04 - val_loss: 0.0069\n",
      "Epoch 228/1000\n",
      "2470/2470 [==============================] - 1s 320us/step - loss: 3.6246e-04 - val_loss: 0.0067\n",
      "Epoch 229/1000\n",
      "2470/2470 [==============================] - 1s 322us/step - loss: 3.6765e-04 - val_loss: 0.0065\n",
      "Epoch 230/1000\n",
      "2470/2470 [==============================] - 1s 319us/step - loss: 3.6360e-04 - val_loss: 0.0068\n",
      "Epoch 231/1000\n",
      "2470/2470 [==============================] - 1s 319us/step - loss: 3.6159e-04 - val_loss: 0.0065\n",
      "Epoch 232/1000\n",
      "2470/2470 [==============================] - 1s 322us/step - loss: 3.6139e-04 - val_loss: 0.0067\n",
      "Epoch 233/1000\n",
      "2470/2470 [==============================] - 1s 311us/step - loss: 3.6161e-04 - val_loss: 0.0067\n",
      "Epoch 234/1000\n",
      "2470/2470 [==============================] - 1s 321us/step - loss: 3.6107e-04 - val_loss: 0.0066\n",
      "Epoch 235/1000\n",
      "2470/2470 [==============================] - 1s 323us/step - loss: 3.6229e-04 - val_loss: 0.0070\n",
      "Epoch 236/1000\n",
      "2470/2470 [==============================] - 1s 343us/step - loss: 3.6613e-04 - val_loss: 0.0065\n",
      "Epoch 237/1000\n",
      "2470/2470 [==============================] - 1s 342us/step - loss: 3.6178e-04 - val_loss: 0.0062\n",
      "Epoch 238/1000\n",
      "2470/2470 [==============================] - 1s 351us/step - loss: 3.5907e-04 - val_loss: 0.0071\n",
      "Epoch 239/1000\n",
      "2470/2470 [==============================] - 1s 328us/step - loss: 3.5977e-04 - val_loss: 0.0066\n",
      "Epoch 240/1000\n",
      "2470/2470 [==============================] - 1s 322us/step - loss: 3.5946e-04 - val_loss: 0.0064\n",
      "Epoch 241/1000\n",
      "2470/2470 [==============================] - 1s 306us/step - loss: 3.5844e-04 - val_loss: 0.0065\n",
      "Epoch 242/1000\n",
      "2470/2470 [==============================] - 1s 327us/step - loss: 3.5882e-04 - val_loss: 0.0065\n",
      "Epoch 243/1000\n",
      "2470/2470 [==============================] - 1s 331us/step - loss: 3.5991e-04 - val_loss: 0.0066\n",
      "Epoch 244/1000\n",
      "2470/2470 [==============================] - 1s 330us/step - loss: 3.5668e-04 - val_loss: 0.0063\n",
      "Epoch 245/1000\n",
      "2470/2470 [==============================] - 1s 315us/step - loss: 3.5816e-04 - val_loss: 0.0064\n",
      "Epoch 246/1000\n",
      "2470/2470 [==============================] - 1s 327us/step - loss: 3.5821e-04 - val_loss: 0.0062\n",
      "Epoch 247/1000\n",
      "2470/2470 [==============================] - 1s 298us/step - loss: 3.5730e-04 - val_loss: 0.0063\n",
      "Epoch 248/1000\n",
      "2470/2470 [==============================] - 1s 299us/step - loss: 3.5804e-04 - val_loss: 0.0067\n",
      "Epoch 249/1000\n",
      "2470/2470 [==============================] - 1s 322us/step - loss: 3.5940e-04 - val_loss: 0.0069\n",
      "Epoch 250/1000\n",
      "2470/2470 [==============================] - 1s 317us/step - loss: 3.5632e-04 - val_loss: 0.0064\n",
      "Epoch 251/1000\n",
      "2470/2470 [==============================] - 1s 320us/step - loss: 3.5702e-04 - val_loss: 0.0065\n",
      "Epoch 252/1000\n",
      "2470/2470 [==============================] - 1s 332us/step - loss: 3.5537e-04 - val_loss: 0.0062\n",
      "Epoch 253/1000\n",
      "2470/2470 [==============================] - 1s 323us/step - loss: 3.5561e-04 - val_loss: 0.0061\n",
      "Epoch 254/1000\n",
      "2470/2470 [==============================] - 1s 322us/step - loss: 3.5759e-04 - val_loss: 0.0061\n",
      "Epoch 255/1000\n",
      "2470/2470 [==============================] - 1s 324us/step - loss: 3.5616e-04 - val_loss: 0.0065\n",
      "Epoch 256/1000\n",
      "2470/2470 [==============================] - 1s 347us/step - loss: 3.5574e-04 - val_loss: 0.0063\n",
      "Epoch 257/1000\n",
      "2470/2470 [==============================] - 1s 329us/step - loss: 3.5287e-04 - val_loss: 0.0064\n",
      "Epoch 258/1000\n",
      "2470/2470 [==============================] - 1s 336us/step - loss: 3.5737e-04 - val_loss: 0.0067\n",
      "Epoch 259/1000\n",
      "2470/2470 [==============================] - 1s 323us/step - loss: 3.5439e-04 - val_loss: 0.0062\n",
      "Epoch 260/1000\n",
      "2470/2470 [==============================] - 1s 341us/step - loss: 3.5377e-04 - val_loss: 0.0058\n",
      "Epoch 261/1000\n",
      "2470/2470 [==============================] - 1s 326us/step - loss: 3.5386e-04 - val_loss: 0.0063\n",
      "Epoch 262/1000\n",
      "2470/2470 [==============================] - 1s 314us/step - loss: 3.5277e-04 - val_loss: 0.0060\n",
      "Epoch 263/1000\n",
      "2470/2470 [==============================] - 1s 331us/step - loss: 3.5394e-04 - val_loss: 0.0063\n",
      "Epoch 264/1000\n",
      "2470/2470 [==============================] - 1s 333us/step - loss: 3.5210e-04 - val_loss: 0.0064\n",
      "Epoch 265/1000\n",
      "2470/2470 [==============================] - 1s 325us/step - loss: 3.5103e-04 - val_loss: 0.0060\n",
      "Epoch 266/1000\n",
      "2470/2470 [==============================] - 1s 323us/step - loss: 3.5192e-04 - val_loss: 0.0064\n",
      "Epoch 267/1000\n",
      "2470/2470 [==============================] - 1s 324us/step - loss: 3.5147e-04 - val_loss: 0.0060\n",
      "Epoch 268/1000\n",
      "2470/2470 [==============================] - 1s 265us/step - loss: 3.5096e-04 - val_loss: 0.0063\n",
      "Epoch 269/1000\n",
      "2470/2470 [==============================] - 1s 307us/step - loss: 3.5023e-04 - val_loss: 0.0061\n",
      "Epoch 270/1000\n",
      "2470/2470 [==============================] - 1s 322us/step - loss: 3.4945e-04 - val_loss: 0.0059\n",
      "Epoch 271/1000\n",
      "2470/2470 [==============================] - 1s 328us/step - loss: 3.5146e-04 - val_loss: 0.0065\n",
      "Epoch 272/1000\n",
      "2470/2470 [==============================] - 1s 319us/step - loss: 3.5122e-04 - val_loss: 0.0061\n",
      "Epoch 273/1000\n",
      "2470/2470 [==============================] - 1s 319us/step - loss: 3.5141e-04 - val_loss: 0.0061\n",
      "Epoch 274/1000\n",
      "2470/2470 [==============================] - 1s 336us/step - loss: 3.4962e-04 - val_loss: 0.0058\n",
      "Epoch 275/1000\n",
      "2470/2470 [==============================] - 1s 317us/step - loss: 3.4810e-04 - val_loss: 0.0062\n",
      "Epoch 276/1000\n",
      "2470/2470 [==============================] - 1s 293us/step - loss: 3.5098e-04 - val_loss: 0.0060\n",
      "Epoch 277/1000\n",
      "2470/2470 [==============================] - 1s 318us/step - loss: 3.4893e-04 - val_loss: 0.0062\n",
      "Epoch 278/1000\n",
      "2470/2470 [==============================] - 1s 307us/step - loss: 3.4735e-04 - val_loss: 0.0057\n",
      "Epoch 279/1000\n",
      "2470/2470 [==============================] - 1s 304us/step - loss: 3.4894e-04 - val_loss: 0.0057\n",
      "Epoch 280/1000\n",
      "2470/2470 [==============================] - 1s 330us/step - loss: 3.4921e-04 - val_loss: 0.0057\n",
      "Epoch 281/1000\n",
      "2470/2470 [==============================] - 1s 342us/step - loss: 3.4863e-04 - val_loss: 0.0064\n",
      "Epoch 282/1000\n",
      "2470/2470 [==============================] - 1s 401us/step - loss: 3.4792e-04 - val_loss: 0.0057\n",
      "Epoch 283/1000\n",
      "2470/2470 [==============================] - 1s 367us/step - loss: 3.4693e-04 - val_loss: 0.0061\n",
      "Epoch 284/1000\n",
      "2470/2470 [==============================] - 1s 337us/step - loss: 3.4577e-04 - val_loss: 0.0061\n",
      "Epoch 285/1000\n",
      "2470/2470 [==============================] - 1s 344us/step - loss: 3.4569e-04 - val_loss: 0.0056\n",
      "Epoch 286/1000\n",
      "2470/2470 [==============================] - 1s 425us/step - loss: 3.4837e-04 - val_loss: 0.0061\n",
      "Epoch 287/1000\n",
      "2470/2470 [==============================] - 1s 287us/step - loss: 3.4669e-04 - val_loss: 0.0058\n",
      "Epoch 288/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2470/2470 [==============================] - 1s 297us/step - loss: 3.4334e-04 - val_loss: 0.0057\n",
      "Epoch 289/1000\n",
      "2470/2470 [==============================] - 1s 318us/step - loss: 3.4627e-04 - val_loss: 0.0058\n",
      "Epoch 290/1000\n",
      "2470/2470 [==============================] - 1s 319us/step - loss: 3.4389e-04 - val_loss: 0.0056\n",
      "Epoch 291/1000\n",
      "2470/2470 [==============================] - 1s 294us/step - loss: 3.4974e-04 - val_loss: 0.0064\n",
      "Epoch 292/1000\n",
      "2470/2470 [==============================] - 1s 325us/step - loss: 3.4665e-04 - val_loss: 0.0058\n",
      "Epoch 293/1000\n",
      "2470/2470 [==============================] - 1s 328us/step - loss: 3.4294e-04 - val_loss: 0.0056\n",
      "Epoch 294/1000\n",
      "2470/2470 [==============================] - 1s 328us/step - loss: 3.4510e-04 - val_loss: 0.0057\n",
      "Epoch 295/1000\n",
      "2470/2470 [==============================] - 1s 265us/step - loss: 3.4571e-04 - val_loss: 0.0060\n",
      "Epoch 296/1000\n",
      "2470/2470 [==============================] - 1s 318us/step - loss: 3.4282e-04 - val_loss: 0.0058\n",
      "Epoch 297/1000\n",
      "2470/2470 [==============================] - 1s 318us/step - loss: 3.4279e-04 - val_loss: 0.0062\n",
      "Epoch 298/1000\n",
      "2470/2470 [==============================] - 1s 322us/step - loss: 3.4279e-04 - val_loss: 0.0057\n",
      "Epoch 299/1000\n",
      "2470/2470 [==============================] - 1s 320us/step - loss: 3.4338e-04 - val_loss: 0.0057\n",
      "Epoch 300/1000\n",
      "2470/2470 [==============================] - 1s 316us/step - loss: 3.4315e-04 - val_loss: 0.0055\n",
      "Epoch 301/1000\n",
      "2470/2470 [==============================] - 1s 330us/step - loss: 3.4316e-04 - val_loss: 0.0058\n",
      "Epoch 302/1000\n",
      "2470/2470 [==============================] - 1s 340us/step - loss: 3.4476e-04 - val_loss: 0.0058\n",
      "Epoch 303/1000\n",
      "2470/2470 [==============================] - 1s 332us/step - loss: 3.4232e-04 - val_loss: 0.0062\n",
      "Epoch 304/1000\n",
      "2470/2470 [==============================] - 1s 273us/step - loss: 3.4047e-04 - val_loss: 0.0056\n",
      "Epoch 305/1000\n",
      "2470/2470 [==============================] - 1s 278us/step - loss: 3.4067e-04 - val_loss: 0.0059\n",
      "Epoch 306/1000\n",
      "2470/2470 [==============================] - 1s 293us/step - loss: 3.4881e-04 - val_loss: 0.0056\n",
      "Epoch 307/1000\n",
      "2470/2470 [==============================] - 1s 322us/step - loss: 3.4360e-04 - val_loss: 0.0056\n",
      "Epoch 308/1000\n",
      "2470/2470 [==============================] - 1s 319us/step - loss: 3.4294e-04 - val_loss: 0.0056\n",
      "Epoch 309/1000\n",
      "2470/2470 [==============================] - 1s 326us/step - loss: 3.3912e-04 - val_loss: 0.0056\n",
      "Epoch 310/1000\n",
      "2470/2470 [==============================] - 1s 315us/step - loss: 3.3944e-04 - val_loss: 0.0053\n",
      "Epoch 311/1000\n",
      "2470/2470 [==============================] - 1s 324us/step - loss: 3.4229e-04 - val_loss: 0.0060\n",
      "Epoch 312/1000\n",
      "2470/2470 [==============================] - 1s 317us/step - loss: 3.3986e-04 - val_loss: 0.0058\n",
      "Epoch 313/1000\n",
      "2470/2470 [==============================] - 1s 280us/step - loss: 3.4067e-04 - val_loss: 0.0051\n",
      "Epoch 314/1000\n",
      "2470/2470 [==============================] - 1s 300us/step - loss: 3.3897e-04 - val_loss: 0.0060\n",
      "Epoch 315/1000\n",
      "2470/2470 [==============================] - 1s 299us/step - loss: 3.3753e-04 - val_loss: 0.0051\n",
      "Epoch 316/1000\n",
      "2470/2470 [==============================] - 1s 304us/step - loss: 3.3752e-04 - val_loss: 0.0051\n",
      "Epoch 317/1000\n",
      "2470/2470 [==============================] - 1s 304us/step - loss: 3.4306e-04 - val_loss: 0.0059\n",
      "Epoch 318/1000\n",
      "2470/2470 [==============================] - 1s 304us/step - loss: 3.4421e-04 - val_loss: 0.0058\n",
      "Epoch 319/1000\n",
      "2470/2470 [==============================] - 1s 323us/step - loss: 3.4106e-04 - val_loss: 0.0049\n",
      "Epoch 320/1000\n",
      "2470/2470 [==============================] - 1s 332us/step - loss: 3.4022e-04 - val_loss: 0.0052\n",
      "Epoch 321/1000\n",
      "2470/2470 [==============================] - 1s 341us/step - loss: 3.3658e-04 - val_loss: 0.0052\n",
      "Epoch 322/1000\n",
      "2470/2470 [==============================] - 1s 322us/step - loss: 3.3645e-04 - val_loss: 0.0055\n",
      "Epoch 323/1000\n",
      "2470/2470 [==============================] - 1s 331us/step - loss: 3.3751e-04 - val_loss: 0.0056\n",
      "Epoch 324/1000\n",
      "2470/2470 [==============================] - 1s 319us/step - loss: 3.3476e-04 - val_loss: 0.0055\n",
      "Epoch 325/1000\n",
      "2470/2470 [==============================] - 1s 325us/step - loss: 3.3893e-04 - val_loss: 0.0054\n",
      "Epoch 326/1000\n",
      "2470/2470 [==============================] - 1s 317us/step - loss: 3.3639e-04 - val_loss: 0.0053\n",
      "Epoch 327/1000\n",
      "2470/2470 [==============================] - 1s 308us/step - loss: 3.3439e-04 - val_loss: 0.0052\n",
      "Epoch 328/1000\n",
      "2470/2470 [==============================] - 1s 312us/step - loss: 3.3582e-04 - val_loss: 0.0050\n",
      "Epoch 329/1000\n",
      "2470/2470 [==============================] - 1s 302us/step - loss: 3.3892e-04 - val_loss: 0.0054\n",
      "Epoch 330/1000\n",
      "2470/2470 [==============================] - 1s 301us/step - loss: 3.3603e-04 - val_loss: 0.0052\n",
      "Epoch 331/1000\n",
      "2470/2470 [==============================] - 1s 320us/step - loss: 3.3671e-04 - val_loss: 0.0055\n",
      "Epoch 332/1000\n",
      "2470/2470 [==============================] - 1s 298us/step - loss: 3.3358e-04 - val_loss: 0.0055\n",
      "Epoch 333/1000\n",
      "2470/2470 [==============================] - 1s 260us/step - loss: 3.3694e-04 - val_loss: 0.0051\n",
      "Epoch 334/1000\n",
      "2470/2470 [==============================] - 1s 284us/step - loss: 3.3517e-04 - val_loss: 0.0053\n",
      "Epoch 335/1000\n",
      "2470/2470 [==============================] - 1s 317us/step - loss: 3.3432e-04 - val_loss: 0.0053\n",
      "Epoch 336/1000\n",
      "2470/2470 [==============================] - 1s 320us/step - loss: 3.3301e-04 - val_loss: 0.0055\n",
      "Epoch 337/1000\n",
      "2470/2470 [==============================] - 1s 321us/step - loss: 3.3128e-04 - val_loss: 0.0050\n",
      "Epoch 338/1000\n",
      "2470/2470 [==============================] - 1s 324us/step - loss: 3.3539e-04 - val_loss: 0.0055\n",
      "Epoch 339/1000\n",
      "2470/2470 [==============================] - 1s 317us/step - loss: 3.3190e-04 - val_loss: 0.0054\n",
      "Epoch 340/1000\n",
      "2470/2470 [==============================] - 1s 320us/step - loss: 3.3250e-04 - val_loss: 0.0050\n",
      "Epoch 341/1000\n",
      "2470/2470 [==============================] - 1s 322us/step - loss: 3.3243e-04 - val_loss: 0.0054\n",
      "Epoch 342/1000\n",
      "2470/2470 [==============================] - 1s 308us/step - loss: 3.3100e-04 - val_loss: 0.0053\n",
      "Epoch 343/1000\n",
      "2470/2470 [==============================] - 1s 316us/step - loss: 3.3191e-04 - val_loss: 0.0052\n",
      "Epoch 344/1000\n",
      "2470/2470 [==============================] - 1s 320us/step - loss: 3.3515e-04 - val_loss: 0.0054\n",
      "Epoch 345/1000\n",
      "2470/2470 [==============================] - 1s 319us/step - loss: 3.3172e-04 - val_loss: 0.0055\n",
      "Epoch 346/1000\n",
      "2470/2470 [==============================] - 1s 330us/step - loss: 3.3346e-04 - val_loss: 0.0051\n",
      "Epoch 347/1000\n",
      "2470/2470 [==============================] - 1s 330us/step - loss: 3.3129e-04 - val_loss: 0.0052\n",
      "Epoch 348/1000\n",
      "2470/2470 [==============================] - 1s 332us/step - loss: 3.3010e-04 - val_loss: 0.0049\n",
      "Epoch 349/1000\n",
      "2470/2470 [==============================] - 1s 337us/step - loss: 3.3000e-04 - val_loss: 0.0052\n",
      "Epoch 350/1000\n",
      "2470/2470 [==============================] - 1s 307us/step - loss: 3.3076e-04 - val_loss: 0.0055\n",
      "Epoch 351/1000\n",
      "2470/2470 [==============================] - 1s 278us/step - loss: 3.3183e-04 - val_loss: 0.0047\n",
      "Epoch 352/1000\n",
      "2470/2470 [==============================] - 1s 358us/step - loss: 3.3099e-04 - val_loss: 0.0047\n",
      "Epoch 353/1000\n",
      "2470/2470 [==============================] - 1s 318us/step - loss: 3.2961e-04 - val_loss: 0.0052\n",
      "Epoch 354/1000\n",
      "2470/2470 [==============================] - 1s 391us/step - loss: 3.2936e-04 - val_loss: 0.0051\n",
      "Epoch 355/1000\n",
      "2470/2470 [==============================] - 1s 352us/step - loss: 3.2589e-04 - val_loss: 0.0050\n",
      "Epoch 356/1000\n",
      "2470/2470 [==============================] - 1s 338us/step - loss: 3.2736e-04 - val_loss: 0.0051\n",
      "Epoch 357/1000\n",
      "2470/2470 [==============================] - 1s 313us/step - loss: 3.2997e-04 - val_loss: 0.0054\n",
      "Epoch 358/1000\n",
      "2470/2470 [==============================] - 1s 327us/step - loss: 3.2927e-04 - val_loss: 0.0051\n",
      "Epoch 359/1000\n",
      "2470/2470 [==============================] - 1s 321us/step - loss: 3.2910e-04 - val_loss: 0.0049\n",
      "Epoch 360/1000\n",
      "2470/2470 [==============================] - 1s 321us/step - loss: 3.2430e-04 - val_loss: 0.0048\n",
      "Epoch 361/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2470/2470 [==============================] - 1s 279us/step - loss: 3.3015e-04 - val_loss: 0.0050\n",
      "Epoch 362/1000\n",
      "2470/2470 [==============================] - 1s 315us/step - loss: 3.2671e-04 - val_loss: 0.0051\n",
      "Epoch 363/1000\n",
      "2470/2470 [==============================] - 1s 326us/step - loss: 3.2469e-04 - val_loss: 0.0052\n",
      "Epoch 364/1000\n",
      "2470/2470 [==============================] - 1s 308us/step - loss: 3.2318e-04 - val_loss: 0.0049\n",
      "Epoch 365/1000\n",
      "2470/2470 [==============================] - 1s 290us/step - loss: 3.2366e-04 - val_loss: 0.0052\n",
      "Epoch 366/1000\n",
      "2470/2470 [==============================] - 1s 293us/step - loss: 3.2489e-04 - val_loss: 0.0047\n",
      "Epoch 367/1000\n",
      "2470/2470 [==============================] - 1s 283us/step - loss: 3.2321e-04 - val_loss: 0.0048\n",
      "Epoch 368/1000\n",
      "2470/2470 [==============================] - 1s 313us/step - loss: 3.2163e-04 - val_loss: 0.0049\n",
      "Epoch 369/1000\n",
      "2470/2470 [==============================] - 1s 318us/step - loss: 3.2110e-04 - val_loss: 0.0049\n",
      "Epoch 370/1000\n",
      "2470/2470 [==============================] - 1s 336us/step - loss: 3.2262e-04 - val_loss: 0.0047\n",
      "Epoch 371/1000\n",
      "2470/2470 [==============================] - 1s 369us/step - loss: 3.2244e-04 - val_loss: 0.0047\n",
      "Epoch 372/1000\n",
      "2470/2470 [==============================] - 1s 290us/step - loss: 3.2266e-04 - val_loss: 0.0051\n",
      "Epoch 373/1000\n",
      "2470/2470 [==============================] - 1s 317us/step - loss: 3.2090e-04 - val_loss: 0.0049\n",
      "Epoch 374/1000\n",
      "2470/2470 [==============================] - 1s 325us/step - loss: 3.2261e-04 - val_loss: 0.0051\n",
      "Epoch 375/1000\n",
      "2470/2470 [==============================] - 1s 329us/step - loss: 3.1975e-04 - val_loss: 0.0049\n",
      "Epoch 376/1000\n",
      "2470/2470 [==============================] - 1s 290us/step - loss: 3.2012e-04 - val_loss: 0.0042\n",
      "Epoch 377/1000\n",
      "2470/2470 [==============================] - 1s 322us/step - loss: 3.2080e-04 - val_loss: 0.0049\n",
      "Epoch 378/1000\n",
      "2470/2470 [==============================] - 1s 317us/step - loss: 3.2034e-04 - val_loss: 0.0048\n",
      "Epoch 379/1000\n",
      "2470/2470 [==============================] - 1s 306us/step - loss: 3.2161e-04 - val_loss: 0.0049\n",
      "Epoch 380/1000\n",
      "2470/2470 [==============================] - 1s 297us/step - loss: 3.1909e-04 - val_loss: 0.0054\n",
      "Epoch 381/1000\n",
      "2470/2470 [==============================] - 1s 314us/step - loss: 3.1955e-04 - val_loss: 0.0046\n",
      "Epoch 382/1000\n",
      "2470/2470 [==============================] - 1s 281us/step - loss: 3.1963e-04 - val_loss: 0.0051\n",
      "Epoch 383/1000\n",
      "2470/2470 [==============================] - 1s 318us/step - loss: 3.1971e-04 - val_loss: 0.0051\n",
      "Epoch 384/1000\n",
      "2470/2470 [==============================] - 1s 322us/step - loss: 3.1678e-04 - val_loss: 0.0050\n",
      "Epoch 385/1000\n",
      "2470/2470 [==============================] - 1s 292us/step - loss: 3.1489e-04 - val_loss: 0.0044\n",
      "Epoch 386/1000\n",
      "2470/2470 [==============================] - 1s 311us/step - loss: 3.1651e-04 - val_loss: 0.0050\n",
      "Epoch 387/1000\n",
      "2470/2470 [==============================] - 1s 305us/step - loss: 3.1744e-04 - val_loss: 0.0050\n",
      "Epoch 388/1000\n",
      "2470/2470 [==============================] - 1s 295us/step - loss: 3.1595e-04 - val_loss: 0.0049\n",
      "Epoch 389/1000\n",
      "2470/2470 [==============================] - 1s 318us/step - loss: 3.1671e-04 - val_loss: 0.0049\n",
      "Epoch 390/1000\n",
      "2470/2470 [==============================] - 1s 319us/step - loss: 3.1692e-04 - val_loss: 0.0046\n",
      "Epoch 391/1000\n",
      "2470/2470 [==============================] - 1s 319us/step - loss: 3.1728e-04 - val_loss: 0.0048\n",
      "Epoch 392/1000\n",
      "2470/2470 [==============================] - 1s 316us/step - loss: 3.1411e-04 - val_loss: 0.0047\n",
      "Epoch 393/1000\n",
      "2470/2470 [==============================] - 1s 319us/step - loss: 3.1895e-04 - val_loss: 0.0050\n",
      "Epoch 394/1000\n",
      "2470/2470 [==============================] - 1s 327us/step - loss: 3.1604e-04 - val_loss: 0.0046\n",
      "Epoch 395/1000\n",
      "2470/2470 [==============================] - 1s 316us/step - loss: 3.1441e-04 - val_loss: 0.0046\n",
      "Epoch 396/1000\n",
      "2470/2470 [==============================] - 1s 324us/step - loss: 3.1432e-04 - val_loss: 0.0050\n",
      "Epoch 397/1000\n",
      "2470/2470 [==============================] - 1s 319us/step - loss: 3.1342e-04 - val_loss: 0.0047\n",
      "Epoch 398/1000\n",
      "2470/2470 [==============================] - 1s 320us/step - loss: 3.1383e-04 - val_loss: 0.0044\n",
      "Epoch 399/1000\n",
      "2470/2470 [==============================] - 1s 319us/step - loss: 3.1247e-04 - val_loss: 0.0049\n",
      "Epoch 400/1000\n",
      "2470/2470 [==============================] - 1s 311us/step - loss: 3.1247e-04 - val_loss: 0.0051\n",
      "Epoch 401/1000\n",
      "2470/2470 [==============================] - 1s 323us/step - loss: 3.1707e-04 - val_loss: 0.0048\n",
      "Epoch 402/1000\n",
      "2470/2470 [==============================] - 1s 323us/step - loss: 3.1405e-04 - val_loss: 0.0044\n",
      "Epoch 403/1000\n",
      "2470/2470 [==============================] - 1s 367us/step - loss: 3.1804e-04 - val_loss: 0.0053\n",
      "Epoch 404/1000\n",
      "2470/2470 [==============================] - 1s 325us/step - loss: 3.1616e-04 - val_loss: 0.0052\n",
      "Epoch 405/1000\n",
      "2470/2470 [==============================] - 1s 317us/step - loss: 3.1337e-04 - val_loss: 0.0044\n",
      "Epoch 406/1000\n",
      "2470/2470 [==============================] - 1s 315us/step - loss: 3.1105e-04 - val_loss: 0.0044\n",
      "Epoch 407/1000\n",
      "2470/2470 [==============================] - 1s 318us/step - loss: 3.1114e-04 - val_loss: 0.0051\n",
      "Epoch 408/1000\n",
      "2470/2470 [==============================] - 1s 313us/step - loss: 3.1521e-04 - val_loss: 0.0049\n",
      "Epoch 409/1000\n",
      "2470/2470 [==============================] - 1s 315us/step - loss: 3.1180e-04 - val_loss: 0.0047\n",
      "Epoch 410/1000\n",
      "2470/2470 [==============================] - 1s 320us/step - loss: 3.1132e-04 - val_loss: 0.0043\n",
      "Epoch 411/1000\n",
      "2470/2470 [==============================] - 1s 339us/step - loss: 3.0974e-04 - val_loss: 0.0049\n",
      "Epoch 412/1000\n",
      "2470/2470 [==============================] - 1s 332us/step - loss: 3.1256e-04 - val_loss: 0.0045\n",
      "Epoch 413/1000\n",
      "2470/2470 [==============================] - 1s 344us/step - loss: 3.0989e-04 - val_loss: 0.0048\n",
      "Epoch 414/1000\n",
      "2470/2470 [==============================] - 1s 320us/step - loss: 3.1468e-04 - val_loss: 0.0051\n",
      "Epoch 415/1000\n",
      "2470/2470 [==============================] - 1s 329us/step - loss: 3.1109e-04 - val_loss: 0.0044\n",
      "Epoch 416/1000\n",
      "2470/2470 [==============================] - 1s 319us/step - loss: 3.1190e-04 - val_loss: 0.0045\n",
      "Epoch 417/1000\n",
      "2470/2470 [==============================] - 1s 319us/step - loss: 3.1191e-04 - val_loss: 0.0047\n",
      "Epoch 418/1000\n",
      "2470/2470 [==============================] - 1s 317us/step - loss: 3.1000e-04 - val_loss: 0.0044\n",
      "Epoch 419/1000\n",
      "2470/2470 [==============================] - 1s 327us/step - loss: 3.1032e-04 - val_loss: 0.0045\n",
      "Epoch 420/1000\n",
      "2470/2470 [==============================] - 1s 318us/step - loss: 3.0695e-04 - val_loss: 0.0045\n",
      "Epoch 421/1000\n",
      "2470/2470 [==============================] - 1s 318us/step - loss: 3.0458e-04 - val_loss: 0.0046\n",
      "Epoch 422/1000\n",
      "2470/2470 [==============================] - 1s 323us/step - loss: 3.0539e-04 - val_loss: 0.0048\n",
      "Epoch 423/1000\n",
      "2470/2470 [==============================] - 1s 322us/step - loss: 3.0592e-04 - val_loss: 0.0049\n",
      "Epoch 424/1000\n",
      "2470/2470 [==============================] - 1s 314us/step - loss: 3.0731e-04 - val_loss: 0.0047\n",
      "Epoch 425/1000\n",
      "2470/2470 [==============================] - 1s 320us/step - loss: 3.0432e-04 - val_loss: 0.0046\n",
      "Epoch 426/1000\n",
      "2470/2470 [==============================] - 1s 324us/step - loss: 3.0414e-04 - val_loss: 0.0047\n",
      "Epoch 427/1000\n",
      "2470/2470 [==============================] - 1s 315us/step - loss: 3.0587e-04 - val_loss: 0.0047\n",
      "Epoch 428/1000\n",
      "2470/2470 [==============================] - 1s 323us/step - loss: 3.0798e-04 - val_loss: 0.0047\n",
      "Epoch 429/1000\n",
      "2470/2470 [==============================] - 1s 319us/step - loss: 3.0776e-04 - val_loss: 0.0045\n",
      "Epoch 430/1000\n",
      "2470/2470 [==============================] - 1s 320us/step - loss: 3.0721e-04 - val_loss: 0.0046\n",
      "Epoch 431/1000\n",
      "2470/2470 [==============================] - 1s 326us/step - loss: 3.0381e-04 - val_loss: 0.0045\n",
      "Epoch 432/1000\n",
      "2470/2470 [==============================] - 1s 323us/step - loss: 3.0394e-04 - val_loss: 0.0044\n",
      "Epoch 433/1000\n",
      "2470/2470 [==============================] - 1s 332us/step - loss: 3.0375e-04 - val_loss: 0.0043\n",
      "Epoch 434/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2470/2470 [==============================] - 1s 326us/step - loss: 3.0440e-04 - val_loss: 0.0046\n",
      "Epoch 435/1000\n",
      "2470/2470 [==============================] - 1s 293us/step - loss: 3.0392e-04 - val_loss: 0.0044\n",
      "Epoch 436/1000\n",
      "2470/2470 [==============================] - 1s 309us/step - loss: 3.0471e-04 - val_loss: 0.0046\n",
      "Epoch 437/1000\n",
      "2470/2470 [==============================] - 1s 327us/step - loss: 3.0285e-04 - val_loss: 0.0046\n",
      "Epoch 438/1000\n",
      "2470/2470 [==============================] - 1s 322us/step - loss: 3.0195e-04 - val_loss: 0.0046\n",
      "Epoch 439/1000\n",
      "2470/2470 [==============================] - 1s 317us/step - loss: 3.0361e-04 - val_loss: 0.0047\n",
      "Epoch 440/1000\n",
      "2470/2470 [==============================] - 1s 324us/step - loss: 3.0297e-04 - val_loss: 0.0044\n",
      "Epoch 441/1000\n",
      "2470/2470 [==============================] - 1s 311us/step - loss: 3.0255e-04 - val_loss: 0.0049\n",
      "Epoch 442/1000\n",
      "2470/2470 [==============================] - 1s 318us/step - loss: 3.0293e-04 - val_loss: 0.0050\n",
      "Epoch 443/1000\n",
      "2470/2470 [==============================] - 1s 279us/step - loss: 3.0618e-04 - val_loss: 0.0047\n",
      "Epoch 444/1000\n",
      "2470/2470 [==============================] - 1s 324us/step - loss: 3.0150e-04 - val_loss: 0.0045\n",
      "Epoch 445/1000\n",
      "2470/2470 [==============================] - 1s 323us/step - loss: 3.0447e-04 - val_loss: 0.0047\n",
      "Epoch 446/1000\n",
      "2470/2470 [==============================] - 1s 318us/step - loss: 3.0057e-04 - val_loss: 0.0049\n",
      "Epoch 447/1000\n",
      "2470/2470 [==============================] - 1s 322us/step - loss: 3.0182e-04 - val_loss: 0.0042\n",
      "Epoch 448/1000\n",
      "2470/2470 [==============================] - 1s 296us/step - loss: 3.0114e-04 - val_loss: 0.0044\n",
      "Epoch 449/1000\n",
      "2470/2470 [==============================] - 1s 295us/step - loss: 3.1285e-04 - val_loss: 0.0052\n",
      "Epoch 450/1000\n",
      "2470/2470 [==============================] - 1s 315us/step - loss: 3.0186e-04 - val_loss: 0.0049\n",
      "Epoch 451/1000\n",
      "2470/2470 [==============================] - 1s 333us/step - loss: 3.0154e-04 - val_loss: 0.0044\n",
      "Epoch 452/1000\n",
      "2470/2470 [==============================] - 1s 304us/step - loss: 2.9930e-04 - val_loss: 0.0047\n",
      "Epoch 453/1000\n",
      "2470/2470 [==============================] - 1s 282us/step - loss: 3.0032e-04 - val_loss: 0.0049\n",
      "Epoch 454/1000\n",
      "2470/2470 [==============================] - 1s 322us/step - loss: 3.0137e-04 - val_loss: 0.0046\n",
      "Epoch 455/1000\n",
      "2470/2470 [==============================] - 1s 364us/step - loss: 3.0023e-04 - val_loss: 0.0044\n",
      "Epoch 456/1000\n",
      "2470/2470 [==============================] - 1s 299us/step - loss: 2.9893e-04 - val_loss: 0.0040\n",
      "Epoch 457/1000\n",
      "2470/2470 [==============================] - 1s 370us/step - loss: 3.0978e-04 - val_loss: 0.0045\n",
      "Epoch 458/1000\n",
      "2470/2470 [==============================] - 1s 384us/step - loss: 2.9840e-04 - val_loss: 0.0046\n",
      "Epoch 459/1000\n",
      "2470/2470 [==============================] - 1s 353us/step - loss: 2.9713e-04 - val_loss: 0.0047\n",
      "Epoch 460/1000\n",
      "2470/2470 [==============================] - 1s 337us/step - loss: 3.0370e-04 - val_loss: 0.0043\n",
      "Epoch 461/1000\n",
      "2470/2470 [==============================] - 1s 319us/step - loss: 2.9820e-04 - val_loss: 0.0048\n",
      "Epoch 462/1000\n",
      "2470/2470 [==============================] - 1s 273us/step - loss: 2.9609e-04 - val_loss: 0.0044\n",
      "Epoch 463/1000\n",
      "2470/2470 [==============================] - 1s 324us/step - loss: 3.0537e-04 - val_loss: 0.0048\n",
      "Epoch 464/1000\n",
      "2470/2470 [==============================] - 1s 296us/step - loss: 3.0221e-04 - val_loss: 0.0049\n",
      "Epoch 465/1000\n",
      "2470/2470 [==============================] - 1s 318us/step - loss: 3.0272e-04 - val_loss: 0.0044\n",
      "Epoch 466/1000\n",
      "2470/2470 [==============================] - 1s 296us/step - loss: 2.9979e-04 - val_loss: 0.0044\n",
      "Epoch 467/1000\n",
      "2470/2470 [==============================] - 1s 320us/step - loss: 2.9693e-04 - val_loss: 0.0048\n",
      "Epoch 468/1000\n",
      "2470/2470 [==============================] - 1s 344us/step - loss: 2.9458e-04 - val_loss: 0.0048\n",
      "Epoch 469/1000\n",
      "2470/2470 [==============================] - 1s 337us/step - loss: 2.9958e-04 - val_loss: 0.0046\n",
      "Epoch 470/1000\n",
      "2470/2470 [==============================] - 1s 295us/step - loss: 2.9519e-04 - val_loss: 0.0046\n",
      "Epoch 471/1000\n",
      "2470/2470 [==============================] - 1s 304us/step - loss: 2.9949e-04 - val_loss: 0.0050\n",
      "Epoch 472/1000\n",
      "2470/2470 [==============================] - 1s 273us/step - loss: 2.9829e-04 - val_loss: 0.0045\n",
      "Epoch 473/1000\n",
      "2470/2470 [==============================] - 1s 278us/step - loss: 2.9574e-04 - val_loss: 0.0048\n",
      "Epoch 474/1000\n",
      "2470/2470 [==============================] - 1s 304us/step - loss: 2.9509e-04 - val_loss: 0.0043\n",
      "Epoch 475/1000\n",
      "2470/2470 [==============================] - 1s 312us/step - loss: 2.9598e-04 - val_loss: 0.0045\n",
      "Epoch 476/1000\n",
      "2470/2470 [==============================] - 1s 356us/step - loss: 2.9494e-04 - val_loss: 0.0046\n",
      "Epoch 477/1000\n",
      "2470/2470 [==============================] - 1s 316us/step - loss: 2.9735e-04 - val_loss: 0.0047\n",
      "Epoch 478/1000\n",
      "2470/2470 [==============================] - 1s 305us/step - loss: 2.9738e-04 - val_loss: 0.0046\n",
      "Epoch 00478: early stopping\n"
     ]
    }
   ],
   "source": [
    "data = data_raw.drop(columns=['code','amount','preclose','adj'])\n",
    "data_Aug = augFeatures(data)\n",
    "data_norm = normalize(data_Aug)\n",
    "X,Y=buildData(data_norm)\n",
    "# X_train, X_test, y_train, y_test = splitBuildData(X, Y)\n",
    "\n",
    "# X_train=toNpArray(X_train)\n",
    "# X_test=toNpArray(X_test)\n",
    "# y_train=toNpArray(y_train)\n",
    "# y_test=toNpArray(y_test)\n",
    "\n",
    "X_np=toNpArray(X)\n",
    "Y_np=toNpArray(Y)\n",
    "\n",
    "model = buildManyToManyModel(X_train.shape)\n",
    "cb_loss = EarlyStopping(monitor=\"loss\", patience=10, verbose=1, mode=\"auto\")\n",
    "cb_acc = EarlyStopping(monitor=\"acc\", patience=10, verbose=1, mode=\"auto\")\n",
    "# epochs: 整数。训练模型迭代轮次。一个轮次是在整个 x 或 y 上的一轮迭代。\n",
    "#     请注意，与 initial_epoch 一起，epochs 被理解为 「最终轮次」。\n",
    "#     模型并不是训练了 epochs 轮，而是到第 epochs 轮停止训练。\n",
    "# batch_size: 整数或 None。每次提度更新的样本数。如果未指定，默认为 32.\n",
    "# validation_split: 在 0 和 1 之间浮动。用作验证集的训练数据的比例。\n",
    "#     模型将分出一部分不会被训练的验证数据，并将在每一轮结束时评估这些验证数据的误差和任何其他模型指标。\n",
    "#     验证数据是混洗之前 x 和y 数据的最后一部分样本中。\n",
    "# shuffle: 布尔值（是否在每轮迭代之前混洗数据）或者 字符串 (batch)。\n",
    "#     batch 是处理 HDF5 数据限制的特殊选项，它对一个 batch 内部的数据进行混洗。\n",
    "#     当 steps_per_epoch 非 None 时，这个参数无效。\n",
    "# validation_data: 元组 (x_val，y_val) 或元组 (x_val，y_val，val_sample_weights)，用来评估损失，\n",
    "#     以及在每轮结束时的任何模型度量指标。模型将不会在这个数据上进行训练。\n",
    "#     这个参数会覆盖 validation_split。\n",
    "history=model.fit(X_np, Y_np, epochs=1000, batch_size=128, validation_split=0.15, callbacks=[cb_loss,cb_acc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'acc'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-108-453976992de3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# 绘制训练 & 验证的准确率值\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'acc'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'val_acc'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtitle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Model accuracy'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mylabel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Accuracy'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'acc'"
     ]
    }
   ],
   "source": [
    "# 绘制训练 & 验证的准确率值\n",
    "plt.plot(history.history['acc'])\n",
    "plt.plot(history.history['val_acc'])\n",
    "plt.title('Model accuracy')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x1a2e3ea978>]"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x1a2e3f0710>]"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Model loss')"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'Loss')"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "Text(0.5, 0, 'Epoch')"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x1a2814e7b8>"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1152x360 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 绘制训练 & 验证的损失值\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('Model loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Error when checking model input: the list of Numpy arrays that you are passing to your model is not the size the model expected. Expected to see 1 array(s), but instead got the following list of 10 arrays: [array([[ 0.39517846,  0.38969461,  0.38387677,  0.36796524, -0.01403759,\n         0.46496148,  0.30347321,  0.42867664, -0.50263426],\n       [ 0.38398589,  0.37879719,  0.38958632,  0.37705869, -0.02...",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-110-b5a9e26a9736>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/Volumes/FLASHMEMORY/anaconda3/envs/finance35/lib/python3.5/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, x, batch_size, verbose, steps)\u001b[0m\n\u001b[1;32m   1147\u001b[0m                              'argument.')\n\u001b[1;32m   1148\u001b[0m         \u001b[0;31m# Validate user data.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1149\u001b[0;31m         \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_standardize_user_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1150\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstateful\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1151\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Volumes/FLASHMEMORY/anaconda3/envs/finance35/lib/python3.5/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[0;34m(self, x, y, sample_weight, class_weight, check_array_lengths, batch_size)\u001b[0m\n\u001b[1;32m    749\u001b[0m             \u001b[0mfeed_input_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    750\u001b[0m             \u001b[0mcheck_batch_axis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# Don't enforce the batch size.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 751\u001b[0;31m             exception_prefix='input')\n\u001b[0m\u001b[1;32m    752\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    753\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0my\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Volumes/FLASHMEMORY/anaconda3/envs/finance35/lib/python3.5/site-packages/keras/engine/training_utils.py\u001b[0m in \u001b[0;36mstandardize_input_data\u001b[0;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[1;32m    100\u001b[0m                 \u001b[0;34m'Expected to see '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnames\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' array(s), '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m                 \u001b[0;34m'but instead got the following list of '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 102\u001b[0;31m                 str(len(data)) + ' arrays: ' + str(data)[:200] + '...')\n\u001b[0m\u001b[1;32m    103\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnames\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m             raise ValueError(\n",
      "\u001b[0;31mValueError\u001b[0m: Error when checking model input: the list of Numpy arrays that you are passing to your model is not the size the model expected. Expected to see 1 array(s), but instead got the following list of 10 arrays: [array([[ 0.39517846,  0.38969461,  0.38387677,  0.36796524, -0.01403759,\n         0.46496148,  0.30347321,  0.42867664, -0.50263426],\n       [ 0.38398589,  0.37879719,  0.38958632,  0.37705869, -0.02..."
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "finance_py_35",
   "language": "python",
   "name": "finance35"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
